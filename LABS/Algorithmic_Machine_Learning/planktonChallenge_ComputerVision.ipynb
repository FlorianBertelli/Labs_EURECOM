{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Algorithmic Machine Learning Challenge</h3>\n",
    "<h1>Plankton Image Classification</h1>\n",
    "<hr style=\"height:2px;border:none;color:#333;background-color:#333;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plankton comprises all the organisms freely drifting with ocean currents. These life forms are a critically important piece of oceanic ecosystems, accounting for more than half the primary production on earth and nearly half the total carbon fixed in the global carbon cycle. They also form the foundation of aquatic food webs, including those of large, commercially important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton’s global significance makes their population levels an ideal measure of the health of the world’s oceans and ecosystems.\n",
    "\n",
    "Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of underwater imagery sensors. \n",
    "\n",
    "In this challenge, which was prepared in cooperation with the Laboratoire d’Océanographie de Villefranche, jointly run by Sorbonne Université and CNRS, plankton images were acquired in the bay of Villefranche, weekly since 2013 and manually engineered features were computed on each imaged object. \n",
    "\n",
    "This challenge aims at developing solid approaches to plankton image classification. We will compare methods based on carefully (but manually) engineered features, with “Deep Learning” methods in which features will be learned from image data alone.\n",
    "\n",
    "The purpose of this challenge is for you to learn about the commonly used paradigms when working with computer vision problems. This means you can choose one of the following paths:\n",
    "\n",
    "- Work directly with the provided images, e.g. using a (convolutional) neural network\n",
    "- Work with the supplied features extracted from the images (*native* or *skimage* or both of them)\n",
    "- Extract your own features from the provided images using a technique of your choice\n",
    "\n",
    "You will find a detailed description about the image data and the features at the end of this text.\n",
    "In any case, the choice of the classifier that you decide to work with strongly depends on the choice of features.\n",
    "\n",
    "Please bear in mind that the purpose of this challenge is not simply to find the best-performing model that was released on e.g. Kaggle for a similar problem. You should rather make sure to understand the dificulties that come with this computer vision task. Moreover, you should be able to justify your choice of features/model and be able to explain its advantages and disadvantages for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "<hr style=\"height:1px;border:none;color:#333;background-color:#333;\" />    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond simply producing a well-performing model for making predictions, in this challenge we would like you to start developing your skills as a machine learning scientist.\n",
    "In this regard, your notebook should be structured in such a way as to explore the five following tasks that are expected to be carried out whenever undertaking such a project.\n",
    "The description below each aspect should serve as a guide for your work, but you are strongly encouraged to also explore alternative options and directions. \n",
    "Thinking outside the box will always be rewarded in these challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /mnt/workspace/.local/lib/python3.5/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /mnt/workspace/.local/lib/python3.5/site-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /mnt/workspace/.local/lib/python3.5/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras) (1.14.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: h5py in /mnt/workspace/.local/lib/python3.5/site-packages (from keras) (2.9.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user 'keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from random import randint\n",
    "    \n",
    "from scipy.stats import norm\n",
    "import scipy.ndimage\n",
    "meta = pd.read_csv('/mnt/datasets/plankton/flowcam/meta.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def extract_zip_to_memory(input_zip):\n",
    "    '''\n",
    "    This function extracts the images stored inside the given zip file.\n",
    "    It stores the result in a python dictionary.\n",
    "    \n",
    "    input_zip (string): path to the zip file\n",
    "    \n",
    "    returns (dict): {filename (string): image_file (bytes)}\n",
    "    '''\n",
    "    input_zip=zipfile.ZipFile(input_zip)\n",
    "    return {name: BytesIO(input_zip.read(name)) for name in input_zip.namelist() if name.endswith('.jpg')}\n",
    "\n",
    "\n",
    "img_files = extract_zip_to_memory(\"/mnt/datasets/plankton/flowcam/imgs.zip\")\n",
    "\n",
    "# Display an example image \n",
    "Image.open(img_files['imgs/32738710.jpg'])\n",
    "\n",
    "# Load the image as a numpy array:\n",
    "np_arr = np.array(Image.open(img_files['imgs/32738710.jpg']))\n",
    "\n",
    "# Be aware that the dictionary will occupy roughly 2GB of computer memory!\n",
    "# To free this memory again, run:\n",
    "# del img_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>1. Data Exploration</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first broad component of your notebook should enable you to familiarise yourselves with the given data, an outline of which is given at the end of this challenge specification.\n",
    "\n",
    "What is new in this challenge is that you will be working with image data. Therefore, you should have a look at example images located in the *imgs.zip* file (see description below). If you decide to work with the native or the skimage features, make sure to understand them!\n",
    "\n",
    "Among others, this section should investigate:\n",
    "\n",
    "- Distribution of the different image dimensions (including the number of channels)\n",
    "- Distribution of the different labels that the images are assigned to\n",
    "\n",
    "The image labels are organized in a taxonomy. We will measure the final model performance for the classification into the *level2* categories. Make sure to understand the meaning of this label inside the taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objid</th>\n",
       "      <th>projid</th>\n",
       "      <th>id</th>\n",
       "      <th>status</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>objdate</th>\n",
       "      <th>objtime</th>\n",
       "      <th>depth_min</th>\n",
       "      <th>depth_max</th>\n",
       "      <th>unique_name</th>\n",
       "      <th>lineage</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32756761.0</td>\n",
       "      <td>133</td>\n",
       "      <td>84963</td>\n",
       "      <td>V</td>\n",
       "      <td>43.683333</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>00:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>detritus</td>\n",
       "      <td>/#/not-living/detritus</td>\n",
       "      <td>detritus</td>\n",
       "      <td>detritus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32759364.0</td>\n",
       "      <td>133</td>\n",
       "      <td>84963</td>\n",
       "      <td>V</td>\n",
       "      <td>43.683333</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>00:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>detritus</td>\n",
       "      <td>/#/not-living/detritus</td>\n",
       "      <td>detritus</td>\n",
       "      <td>detritus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32758055.0</td>\n",
       "      <td>133</td>\n",
       "      <td>28299</td>\n",
       "      <td>V</td>\n",
       "      <td>43.683333</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>00:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>Guinardia</td>\n",
       "      <td>/#/living/Eukaryota/Harosa/Stramenopiles/Ochro...</td>\n",
       "      <td>Guinardia</td>\n",
       "      <td>Rhizosolenids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32758988.0</td>\n",
       "      <td>133</td>\n",
       "      <td>92010</td>\n",
       "      <td>V</td>\n",
       "      <td>43.683333</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>00:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>silks</td>\n",
       "      <td>/#/not-living/plastic/other/silks</td>\n",
       "      <td>silks</td>\n",
       "      <td>silks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32760598.0</td>\n",
       "      <td>133</td>\n",
       "      <td>92010</td>\n",
       "      <td>V</td>\n",
       "      <td>43.683333</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>00:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>silks</td>\n",
       "      <td>/#/not-living/plastic/other/silks</td>\n",
       "      <td>silks</td>\n",
       "      <td>silks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        objid  projid     id status   latitude  longitude     objdate  \\\n",
       "0  32756761.0     133  84963      V  43.683333        7.3  2013-09-19   \n",
       "1  32759364.0     133  84963      V  43.683333        7.3  2013-09-19   \n",
       "2  32758055.0     133  28299      V  43.683333        7.3  2013-09-19   \n",
       "3  32758988.0     133  92010      V  43.683333        7.3  2013-09-19   \n",
       "4  32760598.0     133  92010      V  43.683333        7.3  2013-09-19   \n",
       "\n",
       "    objtime  depth_min  depth_max unique_name  \\\n",
       "0  00:09:00          0         75    detritus   \n",
       "1  00:09:00          0         75    detritus   \n",
       "2  00:09:00          0         75   Guinardia   \n",
       "3  00:09:00          0         75       silks   \n",
       "4  00:09:00          0         75       silks   \n",
       "\n",
       "                                             lineage     level1         level2  \n",
       "0                             /#/not-living/detritus   detritus       detritus  \n",
       "1                             /#/not-living/detritus   detritus       detritus  \n",
       "2  /#/living/Eukaryota/Harosa/Stramenopiles/Ochro...  Guinardia  Rhizosolenids  \n",
       "3                  /#/not-living/plastic/other/silks      silks          silks  \n",
       "4                  /#/not-living/plastic/other/silks      silks          silks  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In our data set we have about 243610 images\n"
     ]
    }
   ],
   "source": [
    "#Starting to familiarizing with the given data by fiding its len\n",
    "print(\"In our data set we have about\" , len(img_files) , \"images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different level2 is:  40\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of different level2 is: \", meta.level2.unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level2\n",
      "Bacteriastrum                         12\n",
      "Xystonellidae                         37\n",
      "tempChaetoceros danicus               61\n",
      "Lithodesmioides                       68\n",
      "Cyttarocylis                         100\n",
      "Asterionellopsis                     117\n",
      "Odontella (Mediophyceae)             131\n",
      "centric                              145\n",
      "Ceratocorys horrida                  186\n",
      "Pleurosigma                          191\n",
      "Retaria                              257\n",
      "Coscinodiscids                       334\n",
      "Stenosemella                         357\n",
      "Rhabdonella                          367\n",
      "Annelida                             481\n",
      "Dinophysiales                        525\n",
      "Dictyocysta                          549\n",
      "Hemiaulus                            670\n",
      "egg (other)                          685\n",
      "Undellidae                           710\n",
      "chainlarge                           751\n",
      "Codonaria                            845\n",
      "pollen                              1821\n",
      "artefact                            1849\n",
      "Chaetoceros                         2105\n",
      "Rhizosolenids                       2160\n",
      "Tintinnidiidae                      2227\n",
      "Protoperidinium                     2256\n",
      "Codonellopsis (Dictyocystidae)      2888\n",
      "multiple (other)                    3261\n",
      "rods                                4044\n",
      "Thalassionema                       5117\n",
      "Copepoda                            5141\n",
      "silks                               5629\n",
      "badfocus (artefact)                 7848\n",
      "nauplii (Crustacea)                 9293\n",
      "Neoceratium                        14014\n",
      "feces                              26936\n",
      "detritus                          138439\n",
      "Name: objid, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Identifying the different level twos that are less present and we have to do data augmentatiin on it\n",
    "different_species = meta.groupby(['level2'])['objid'].count()\n",
    "print(different_species.sort_values(ascending=True))\n",
    "\n",
    "#taking the 30 less represented level2\n",
    "little_class = different_species.sort_values(ascending=True)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__ :\n",
    "\n",
    "As we can see we have about 40 different level two, and some are more present than others.\n",
    "Indeed a few are less present and some are very present. This is interesting to notify because \n",
    "in data augmentation we will focus to increase the data size of this level two especially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level1    3334\n",
      "level2    1003\n",
      "dtype: int64\n",
      "level1    2618\n",
      "level2       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Printing null value and droping it \n",
    "null_columns=meta.columns[meta.isnull().any()]\n",
    "print(meta[null_columns].isnull().sum())\n",
    "\n",
    "meta =  meta.dropna(subset=['level2'])\n",
    "print(meta[null_columns].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__ :\n",
    "\n",
    "Here in this cell, we delete the null column because some level 2 are Null ie non described for instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Annelida', 'Asterionellopsis', 'Bacteriastrum', 'Ceratocorys horrida', 'Chaetoceros', 'Codonaria', 'Codonellopsis (Dictyocystidae)', 'Coscinodiscids', 'Cyttarocylis', 'Dictyocysta', 'Dinophysiales', 'Hemiaulus', 'Lithodesmioides', 'Odontella (Mediophyceae)', 'Pleurosigma', 'Protoperidinium', 'Retaria', 'Rhabdonella', 'Rhizosolenids', 'Stenosemella', 'Tintinnidiidae', 'Undellidae', 'Xystonellidae', 'artefact', 'centric', 'chainlarge', 'egg (other)', 'multiple (other)', 'pollen', 'tempChaetoceros danicus']\n"
     ]
    }
   ],
   "source": [
    "little_class = list(little_class.index.sort_values(ascending=True)[:70])\n",
    "print(little_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping useless columns\n",
    "meta = meta.drop(['longitude', 'objdate' ,'objtime' , 'depth_max' , 'depth_min' , 'level1', 'unique_name' , 'lineage', 'id' , 'projid', 'status', 'latitude'] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objid</th>\n",
       "      <th>level2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32756761.0</td>\n",
       "      <td>detritus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32759364.0</td>\n",
       "      <td>detritus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32758055.0</td>\n",
       "      <td>Rhizosolenids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32758988.0</td>\n",
       "      <td>silks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32760598.0</td>\n",
       "      <td>silks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        objid         level2\n",
       "0  32756761.0       detritus\n",
       "1  32759364.0       detritus\n",
       "2  32758055.0  Rhizosolenids\n",
       "3  32758988.0          silks\n",
       "4  32760598.0          silks"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=50x55 at 0x7F8E9312D438>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display an example image \n",
    "Image.open(img_files['imgs/32738710.jpg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max is  (739, 198)\n",
      "The min is (2, 7)\n"
     ]
    }
   ],
   "source": [
    "#Working on the different size of the initial images\n",
    "img_list = np.array(list(img_files.values()))\n",
    "img_shape_list=[]\n",
    "\n",
    "for i in range (len(img_list)):\n",
    "    np_arr = np.array(Image.open(img_list[i]))\n",
    "    img_shape_list+= [np_arr.shape]\n",
    "    \n",
    "print('The max is ' , max(img_shape_list))\n",
    "print(\"The min is\" , min(img_shape_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>243610.000000</td>\n",
       "      <td>243610.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>109.338582</td>\n",
       "      <td>86.628936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>71.930568</td>\n",
       "      <td>62.146386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>135.000000</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>739.000000</td>\n",
       "      <td>972.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1\n",
       "count  243610.000000  243610.000000\n",
       "mean      109.338582      86.628936\n",
       "std        71.930568      62.146386\n",
       "min         2.000000       2.000000\n",
       "25%        61.000000      49.000000\n",
       "50%        87.000000      67.000000\n",
       "75%       135.000000     104.000000\n",
       "max       739.000000     972.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size_DF = pd.DataFrame(img_shape_list)\n",
    "size_DF.hist()\n",
    "size_DF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__:\n",
    "\n",
    "So, as we can see, in the distributions of the size of the image on x and y. The distribution is quite on the little value,\n",
    "as we can see about 50% of the image are less than 87 on X and less than 49 on Y.\n",
    "This two histograms will permit us to choose and size to resize our images in the next part to create a standard 'size' and \n",
    "avoid our CNN to learn from the size of the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>2. Data Pre-processing</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous step should give you a better understanding of which pre-processing is required for the data based on your approach:\n",
    "\n",
    "- If you decide to work with the provided features, some data cleaning may be required to make full use of all the data.\n",
    "- If you decide to extract your own features from the images, you should explain your approach in this section.\n",
    "- If you decide to work directly with the images themselves, preprocessing the images may improve your classification results. In particular, if you work with a neural network the following should be of interest to you:\n",
    "\n",
    "  - Due to the fully-connected layers (that usually come after the convolutional ones), the input needs to have a fixed dimension.\n",
    "  - Data augmentation (image rotation, scaling, cropping, etc. of the existing images) can be used to increase the size of the training data set. This may improve performance especially when little data is available for a particular class.\n",
    "  - Be aware of the computational cost! It might be worth rescaling the images to a smaller size!\n",
    "\n",
    "  All of the operations above are usually realized using a dataloader. This means that you do not need to create a modified version of the dataset and save it to disk. Instead, the dataloader processes the data \"on the fly\" and in-memory before passing it to the network.\n",
    "  \n",
    "    NB: Although aligning image sizes is necessary to train CNNs, this will prevent your classifier from learning about different object sizes as a feature. Additional gains may be achieved when also taking object sizes into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__:\n",
    "    \n",
    "In this part we will do preprocessing, resizing, scaling, rotating, and creating the list in a formal way to be passed in as the training set to our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files = extract_zip_to_memory(\"/mnt/datasets/plankton/flowcam/imgs.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n"
     ]
    }
   ],
   "source": [
    "#THE AIM OF THIS CELL IS TO PERFORM IMAGE RESIZING AND INCREASE THE \n",
    "#DATASET SIZE FOR THE CATEGORIES WHICH ARE NOT WELL PRESENT IN THE DATAST\n",
    "IMG_SIZE = 60\n",
    "\n",
    "j=0\n",
    "i=0\n",
    "img_arrays = {}\n",
    "\n",
    "\n",
    "# the data, split between train and test sets\n",
    "train_ids, test_ids, train_labels, test_labels = train_test_split(meta.objid, meta.level2,stratify=meta.level2, test_size=0.4, random_state=2)\n",
    "\n",
    "\n",
    "x_train, x_test = [] , []\n",
    "\n",
    " ###FOR EACH IMAGE OF THE TRAINING SET :  IE EACH LINE OF OUR DATAFRAME , WE RESIZE AND PERFORM DATA AUGMENTATION\n",
    "for img_id in train_ids:\n",
    "    \n",
    "    #Select the image, performs resizing and add it to our list of image in numpy\n",
    "    img = Image.open(img_files['imgs/' + str(int(img_id)) + '.jpg'])\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
    "    img = np.array(img)\n",
    "    img = np.divide((img),255)\n",
    "    img = img.astype('float32')\n",
    "    x_train.append(img.reshape(IMG_SIZE,IMG_SIZE,1))\n",
    "    #If this image is a part of the less representative class\n",
    "    if ( list(meta.loc[meta['objid'] == img_id, 'level2'])[0] in little_class): \n",
    "\n",
    "        #Getting the specie corresponding to this image to add a corresponding line to the meta\n",
    "        specie = list(meta.loc[meta['objid'] == img_id, 'level2'])[0]\n",
    "         \n",
    "        #Flipping left right\n",
    "        img_arrays[j]=  np.fliplr(img)\n",
    "        \n",
    "\n",
    "        #Flipping Up Down\n",
    "        img_arrays[j+1]=  np.flipud(img)\n",
    "\n",
    "\n",
    "        #Rotating two times with two different angle  \n",
    "        img_arrays[j+2]= scipy.ndimage.rotate(img, float(randint(-30,30)), reshape=False, order=5, mode='nearest')\n",
    "        \n",
    "        img_arrays[j+3]= scipy.ndimage.rotate(img, float(randint(-30,30)), reshape=False, order=5, mode='nearest')\n",
    "        \n",
    "\n",
    "        #Adding a three new lines in our df (one for each modification) with the unique_name corresponding to the original image\n",
    "        df = pd.DataFrame({'objid' :[j,j+1,j+2,j+3], 'level2' :[specie , specie ,specie ,specie]})\n",
    "        meta = meta.append(df, ignore_index=True)  \n",
    "        j+=4\n",
    "       \n",
    "    #To track how our program is advancing\n",
    "    i+=1\n",
    "    if (i%10000==0):\n",
    "        print(i)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__ :\n",
    "\n",
    "\n",
    "In the cells after we realised what is called 'Data Augmentation'. Indeed, n is a way of creating new 'data' with different\n",
    "orientations.The benefits of this are two fold, the first being the ability to generate 'more data' from limited\n",
    "data and secondly it prevents over fitting. We choose to increase only the less present level2 ie the 30 first in term of \n",
    "size. For this, we realise first the split between training data and validation data, because we want to increase only the \n",
    "training set and not the validation set. The operation we have done to increase our training set are, fliping up/down , \n",
    "fliping right/left , and two rotation with a random angle. e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING THE NEW IMAGES TO THE TRAINING SET (ie adding it to training ids and label)\n",
    "for i in range (j):\n",
    "    train_ids= train_ids.append(pd.DataFrame([i]))\n",
    "    train_labels = train_labels.append(meta.level2.iloc[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERFORMING NORMALIZATION ON THE NEW IMAGES ADDED\n",
    "for i in range (j):\n",
    "    img = img_arrays[i].astype('float32')\n",
    "    x_train.append(img.reshape(IMG_SIZE,IMG_SIZE,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PERFORMING RESIZING AND NORMALIZATION ON THE TEST\n",
    "for img_id in test_ids:\n",
    "    #Select the image, performs resizing and add it to our list of image in numpy\n",
    "    img = Image.open(img_files['imgs/' + str(int(img_id)) + '.jpg'])\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
    "    img = np.array(img)\n",
    "    img  = np.divide(img,255)\n",
    "    img = img.astype('float32')\n",
    "    x_test.append(img.reshape(IMG_SIZE,IMG_SIZE,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del img_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'New images to increase the dataset (fliplr, flipud, rotation)')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the modified version of this image for an example\n",
    "for i in range (1,5):\n",
    "    plt.subplot(1,4,i)\n",
    "    plt.imshow(img_arrays[i-1], cmap='Greys')\n",
    "plt.title('New images to increase the dataset (fliplr, flipud, rotation)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New length of our dataset  260747\n",
      "Difference between before and after image processing 18140\n",
      "we augmented the dataframe and image dataset in the same way\n"
     ]
    }
   ],
   "source": [
    "#Checking we increase in a correct way \n",
    "print('New length of our dataset ' ,len(x_test)+len(x_train) )\n",
    "\n",
    "#Checking the difference between the size before and after image processing\n",
    "print('Difference between before and after image processing' ,abs( len(x_train)+ len(x_test) -  (243610 -1003)) )\n",
    "\n",
    "#Checking our dataframe and our list of img increased in the same way\n",
    "if(len(meta.index)- len(x_train)- len(x_test) == 0):\n",
    "    print('we augmented the dataframe and image dataset in the same way')\n",
    "else:\n",
    "    print('!!!!!! MISTAKE !!!!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>3. Model Selection</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most important segment of this challenge involves the selection of a model that can successfully handle the given data and yield sensible predictions.\n",
    "Instead of focusing exclusively on your final chosen model, it is also important to share your thought process in this notebook by additionally describing alternative candidate models.\n",
    "\n",
    "The choice of your model is closely connected to the way you preprocessed the input data.\n",
    "\n",
    "Furthermore, there are other factors which may influence your decision:\n",
    "\n",
    "- What is the model's complexity?\n",
    "- Is the model interpretable?\n",
    "- Is the model capable of handling different data-types?\n",
    "- Does the model return uncertainty estimates along with predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Why this model ?</h5>\n",
    "\n",
    "First of all before deciding to choose to work with a convolutional Neural Network we find it necessary to interest ourselves in all the possibility to do image classification with the raw images after preprocessing. Indeed, Convolutional network although if it's currently a trend, is not the only way to do image classification.\n",
    "\n",
    "First of all, we decided to use convolutional Neural Network (CNN) because of our dataset. Indeed, the data set size is about 250 000 images, so it is enough to have a performant neural network at the end. CNN requires a lot of dataset because if not it overfit because it can function as highly discriminative features even for considerably different data domains and tasks.\n",
    "\n",
    "Moreover, we use this methods because it appears to be more more powerfull than others. Indeed, for instance, we could have used Support Vector Machine (SVM). Before CCNs weren't use SVMs were a good solution. For instance, i can train a SVM with 90x90 features where each feature is the pixel value for a 90x90 image, but in this case with pixel vector I lose a lot of spatial interaction between pixels compared to a CNN. It uses adjacent pixel information to effectively downsample the image first by convolution and then uses a prediction layer at the end.\n",
    "\n",
    "Furthermore, what is interesting with CNN and that there is not much feature selection or preprocessing compared to ther algorithm in image classificaiton like Nearest Neighbours or K-NN, because it we compare handcrafted features with CNN, CNN performance well and it gives better accuracy. It is covering local and global features. It also learns different features from images.\n",
    "\n",
    "Last, why do I choose this type of CNN, there I'm speaking in term of architecture (number of level, or activation function), we've made several test in the following part, concerning the number of layers. Concerning the activation function, I've tried to make a state of the art and try some activation function at the end too. Indeed, for the activation function, the best for multi class classification is the softmax activation as it gives us the probability to be in the corresponding class. The exponential is not adapted at all, and the sigmoid is made essentially for binary classification, so this is not adapted for our case where we got more than 30 classes.\n",
    "\n",
    "\n",
    "Moreover, we've decided to take a simple architecture, 3 levels, to avoid overfitting, because a deeper model means better learning on the training set but more overfitting too. We did that in order to avoid to train during 30 minutes/epoch too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> What is this model's complexity ?</h5>\n",
    "\n",
    "This model of CNN is a quite complex model in term of number of 'things' ie weight or parameters to train, to choose (number of layers, activation function, etc...)There are about 150 000 trainable parameters according to Keeras sum up of our CNN. Indeed, this is characteristic of Neural Network in general, however thank's to Keeras it is really easy to implement neural networks and we just have to focus on parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Is it human readable ?</h5>\n",
    "\n",
    "This is one of the negative point of our algorithm, and of Neural Network in general. This is not human readable very much, especially compared with Decision Tree for instance. However, we care more here on the result and this is the most powerfull machine learning algorithm in this case so human readability does not matter at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Is this model capable of handling different data types ?</h5>\n",
    "\n",
    "This model is capable of handling black and white images (not colored images) because our CNN is trained on B&W images and not RGB. It can support easily various size of images because our algorithm performs resizing at the beggining to obtain a standard size for the CNNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Trying various kind of CNNs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###THE AIM OF THIS PART IS TO IMPLEMENT A FIRST VERSION OF OUR MODEL###\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Generating the training set and test set</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (163704, 60, 60, 1)\n",
      "x_test shape: (97043, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "#For each id in train set add the image corresponding to the list\n",
    "#Same for the test set\n",
    "\n",
    "x_train, x_test = np.array(x_train), np.array(x_test)\n",
    "    \n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (163704, 39)\n",
      "y_test shape: (97043, 39)\n"
     ]
    }
   ],
   "source": [
    "num_classes = meta.level2.unique().size\n",
    "\n",
    "class_matrix = {}\n",
    "c = 0\n",
    "for label in meta.level2.unique():\n",
    "    class_matrix[label] = c\n",
    "    c+=1\n",
    "\n",
    "#Creating y_train and y_test in a categorical form (0 --> [1 , 0 , ... , 0])\n",
    "y_train, y_test = [], []\n",
    "for label in train_labels:\n",
    "    y_train.append(class_matrix[label])\n",
    "for label in test_labels:\n",
    "    y_test.append(class_matrix[label])\n",
    "    \n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del img_arrays\n",
    "del meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Generating different model and comparing their efficiency</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to build the CNN for different activation functions\n",
    "def build_CNN(activation_function , save_file, x_train , y_train , x_test , y_test , epochs , batch_size):\n",
    "    \n",
    "    #\n",
    "    CNN = Sequential()\n",
    "    CNN.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "    CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                     activation='relu'))\n",
    "    CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN.add(Conv2D(64, kernel_size=(3, 3), \n",
    "                     activation='relu'))\n",
    "    CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    CNN.add(Dense(64, activation='relu'))\n",
    "    CNN.add(Dropout(0.5))\n",
    "    CNN.add(Dense(num_classes, activation=activation_function))\n",
    "\n",
    "    #Different case we may have\n",
    "    if (activation_function in ['exponential' , 'softmax']):\n",
    "        print('Using' , activation_function)\n",
    "        CNN.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "    else :    \n",
    "        CNN.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    CNN.summary()\n",
    "\n",
    "    history = CNN.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    #Saving in the file to avoid recomputing\n",
    "    CNN.save(save_file) \n",
    "    return(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 190948 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "190948/190948 [==============================] - 354s 2ms/step - loss: 0.1054 - acc: 0.9642 - val_loss: 0.0563 - val_acc: 0.9804\n",
      "Epoch 2/2\n",
      "190948/190948 [==============================] - 332s 2ms/step - loss: 0.0614 - acc: 0.9808 - val_loss: 0.0500 - val_acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "#Generating a first test with the sigmoid as activation function\n",
    "sigmoid_CNN = build_CNN('sigmoid' , 'sigmoid_file.h5', x_train , y_train , x_test , y_test , 2 , 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To test our function on the multi class classification\n",
    "score=0\n",
    "\n",
    "y_pred = sigmoid_CNN.predict(x_test)\n",
    "b = np.zeros_like(y_pred)\n",
    "b[np.arange(len(y_pred)), y_pred.argmax(1)] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27813\n",
      "Our error rate is  38.21359383372491 %\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "\n",
    "result = b - y_test\n",
    "#Checking if predicted and real class are the same\n",
    "for i in range ((y_test.shape[0])):\n",
    "    if (np.count_nonzero(result[i]) != 0):\n",
    "        score+=1\n",
    "\n",
    "print(score)      \n",
    "print(\"Our error rate is \" , score/y_test.shape[0]*100 , '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using softmax\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 190948 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "190948/190948 [==============================] - 317s 2ms/step - loss: 1.8940 - acc: 0.5609 - val_loss: 1.6222 - val_acc: 0.5865\n",
      "Epoch 2/2\n",
      "190948/190948 [==============================] - 312s 2ms/step - loss: 1.5900 - acc: 0.5908 - val_loss: 1.2653 - val_acc: 0.6287\n"
     ]
    }
   ],
   "source": [
    "#Generating a first test with the softmax as activation function\n",
    "sofmax_CNN = build_CNN('softmax' , 'softmax_file.h5', x_train , y_train , x_test , y_test , 2 , 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To test our function \n",
    "score=0\n",
    "#softmax_CNN = load_model('softmax_file.h5')\n",
    "y_pred = sofmax_CNN.predict(x_test)\n",
    "b = np.zeros_like(y_pred)\n",
    "b[np.arange(len(y_pred)), y_pred.argmax(1)] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27026\n",
      "Our error rate is  37.13229737713477 %\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "\n",
    "result = b - y_test\n",
    "#Checking if predicted and real class are the same\n",
    "for i in range ((y_test.shape[0])):\n",
    "    if (np.count_nonzero(result[i]) != 0):\n",
    "        score+=1\n",
    "\n",
    "print(score)      \n",
    "print(\"Our error rate is \" , score/y_test.shape[0]*100 , '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>4. Parameter Optimisation</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irrespective of your choice, it is highly likely that your model will have one or more (hyper-)parameters that require tuning.\n",
    "There are several techniques for carrying out such a procedure, including cross-validation, Bayesian optimisation, and several others.\n",
    "As before, an analysis into which parameter tuning technique best suits your model is expected before proceeding with the optimisation of your model.\n",
    "\n",
    "If you use a neural network, the optimization of hyperparameters (learning rate, weight decay, etc.) can be a very time-consuming process. In this case, your may decide to carry out smaller experiments and to justify your choice on these preliminary tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Why can we work on a reduced data set ?</h4>\n",
    "\n",
    "In our model we'll train on a reduced model (which is about 0.6, 60% of the total data set). Indeed, we can wonder if we'll loose some usefull data as some level 2 are not really representative. Is it why we use the option stratify of sklearn when doin train_test_to_split to keep the proportion of the different level2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1. Batch size and Learning Rate</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__ :\n",
    "    \n",
    "In this part we worked on the different hyper-parameters in our model, ie for instance, the Learning Rate, The batch size, \n",
    "the Weight Decay , the dropout.\n",
    "\n",
    "First we tried several batch sizes, and we keep the most efficient size in term of computation time, and efficiency.\n",
    "\n",
    "Then we tried several learning rate, between 0.01 and 1000. We found that the most interesting and efficent learning rate was 5.\n",
    "But why ? THe learning rate describes how we evolve to reduce the error (by a big or a little factor). The small ones are not\n",
    "efficient because, they are too low and they need to much epoch to reach the global minimum, and we can also be trapped in\n",
    "a local minimun without being able to flee it. In an other hand, the big ones are to big and didn't manage to reach the global\n",
    "minimum but only its neighbourhood. Here, 5 appears to be a good alternative, a good solution between the big ones and the small ones.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to build the CNN for different activation functions\n",
    "def build_CNN_lr(learning_rate , x_train , y_train , x_test , y_test , epochs , batch_size):\n",
    "    CNN = Sequential()\n",
    "    CNN.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "    CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                     activation='relu'))\n",
    "    CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN.add(Conv2D(64, kernel_size=(3, 3), \n",
    "                     activation='relu'))\n",
    "    CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    CNN.add(Dense(64, activation='relu'))\n",
    "    CNN.add(Dropout(0.5))\n",
    "    CNN.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    CNN.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr = learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    CNN.summary()\n",
    "\n",
    "    history = CNN.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    return(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE IS  0.01 BATCH SIZE IS 128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 190948 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "190948/190948 [==============================] - 320s 2ms/step - loss: 2.2139 - acc: 0.5528 - val_loss: 1.8311 - val_acc: 0.5719\n",
      "Epoch 2/2\n",
      "190948/190948 [==============================] - 317s 2ms/step - loss: 1.9682 - acc: 0.5642 - val_loss: 1.7208 - val_acc: 0.5719\n",
      "LEARNING RATE IS  0.01 BATCH SIZE IS 256\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 190948 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "190948/190948 [==============================] - 323s 2ms/step - loss: 2.3910 - acc: 0.5252 - val_loss: 1.8659 - val_acc: 0.5719\n",
      "Epoch 2/2\n",
      "190948/190948 [==============================] - 322s 2ms/step - loss: 2.0097 - acc: 0.5642 - val_loss: 1.7421 - val_acc: 0.5719\n",
      "LEARNING RATE IS  0.01 BATCH SIZE IS 512\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 190948 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "190948/190948 [==============================] - 333s 2ms/step - loss: 2.6883 - acc: 0.4734 - val_loss: 1.9799 - val_acc: 0.5719\n",
      "Epoch 2/2\n",
      "190948/190948 [==============================] - 332s 2ms/step - loss: 2.1461 - acc: 0.5626 - val_loss: 1.8540 - val_acc: 0.5719\n",
      "LEARNING RATE IS  0.01 BATCH SIZE IS 1028\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 190948 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "190948/190948 [==============================] - 378s 2ms/step - loss: 2.9891 - acc: 0.4262 - val_loss: 2.1545 - val_acc: 0.5719\n",
      "Epoch 2/2\n",
      "190948/190948 [==============================] - 458s 2ms/step - loss: 2.2589 - acc: 0.5645 - val_loss: 1.9571 - val_acc: 0.5719\n"
     ]
    }
   ],
   "source": [
    "#Generating a first test with the softmax as activation function\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {'learning_rate': [ 0.01], 'batch_size': [128,256,512,1028]}\n",
    "resultsDF = pd.DataFrame(columns=['learning_rate', 'batch_size'])\n",
    "\n",
    "parameter_grid = (list(ParameterGrid(param_grid)))\n",
    "for parameter_combination in parameter_grid:\n",
    "    print('LEARNING RATE IS ' , parameter_combination.get('learning_rate') , 'BATCH SIZE IS' , parameter_combination.get('batch_size') )\n",
    "    sofmax_CNN = build_CNN_lr(parameter_combination.get('learning_rate'), x_train , y_train , x_test , y_test , 2 , parameter_combination.get('batch_size'))\n",
    "    del sofmax_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE IS  0.1 BATCH SIZE IS 128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 190948 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "190948/190948 [==============================] - 465s 2ms/step - loss: 1.8496 - acc: 0.5648 - val_loss: 1.4713 - val_acc: 0.5986\n",
      "Epoch 2/2\n",
      "190948/190948 [==============================] - 446s 2ms/step - loss: 1.6141 - acc: 0.5897 - val_loss: 1.3488 - val_acc: 0.6187\n",
      "LEARNING RATE IS  0.1 BATCH SIZE IS 256\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 190948 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "190948/190948 [==============================] - 459s 2ms/step - loss: 1.9326 - acc: 0.5593 - val_loss: 1.5334 - val_acc: 0.5990\n",
      "Epoch 2/2\n",
      "190948/190948 [==============================] - 448s 2ms/step - loss: 1.6703 - acc: 0.5816 - val_loss: 1.3928 - val_acc: 0.6053\n",
      "LEARNING RATE IS  0.1 BATCH SIZE IS 512\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 190948 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "190948/190948 [==============================] - 459s 2ms/step - loss: 2.0163 - acc: 0.5521 - val_loss: 1.5800 - val_acc: 0.5719\n",
      "Epoch 2/2\n",
      "190948/190948 [==============================] - 443s 2ms/step - loss: 1.7510 - acc: 0.5721 - val_loss: 1.5127 - val_acc: 0.6019\n",
      "LEARNING RATE IS  0.1 BATCH SIZE IS 1028\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 190948 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "190948/190948 [==============================] - 473s 2ms/step - loss: 2.1812 - acc: 0.5449 - val_loss: 1.7099 - val_acc: 0.5719\n",
      "Epoch 2/2\n",
      "190948/190948 [==============================] - 471s 2ms/step - loss: 1.8680 - acc: 0.5626 - val_loss: 1.6379 - val_acc: 0.5719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid = {'learning_rate': [0.1], 'batch_size': [128,256,512,1028]}\n",
    "resultsDF = pd.DataFrame(columns=['learning_rate', 'batch_size'])\n",
    "\n",
    "parameter_grid = (list(ParameterGrid(param_grid)))\n",
    "for parameter_combination in parameter_grid:\n",
    "    print('LEARNING RATE IS ' , parameter_combination.get('learning_rate') , 'BATCH SIZE IS' , parameter_combination.get('batch_size') )\n",
    "    sofmax_CNN = build_CNN_lr(parameter_combination.get('learning_rate'), x_train , y_train , x_test , y_test , 2 , parameter_combination.get('batch_size'))\n",
    "    del sofmax_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE IS  0.1 BATCH SIZE IS 128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 163704 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "163704/163704 [==============================] - 305s 2ms/step - loss: 1.9145 - acc: 0.5552 - val_loss: 1.4678 - val_acc: 0.5982\n",
      "Epoch 2/2\n",
      "163704/163704 [==============================] - 287s 2ms/step - loss: 1.6621 - acc: 0.5833 - val_loss: 1.3577 - val_acc: 0.6200\n",
      "LEARNING RATE IS  1 BATCH SIZE IS 128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 163704 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "163704/163704 [==============================] - 291s 2ms/step - loss: 1.6433 - acc: 0.5848 - val_loss: 1.1406 - val_acc: 0.6516\n",
      "Epoch 2/2\n",
      "163704/163704 [==============================] - 287s 2ms/step - loss: 1.3363 - acc: 0.6359 - val_loss: 0.9896 - val_acc: 0.6863\n",
      "LEARNING RATE IS  5 BATCH SIZE IS 128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 163704 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "163704/163704 [==============================] - 292s 2ms/step - loss: 1.6279 - acc: 0.5835 - val_loss: 1.1241 - val_acc: 0.6668\n",
      "Epoch 2/2\n",
      "163704/163704 [==============================] - 288s 2ms/step - loss: 1.2884 - acc: 0.6419 - val_loss: 0.9840 - val_acc: 0.6911\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {'learning_rate': [0.1 , 1 ,5 ], 'batch_size': [128]}\n",
    "\n",
    "\n",
    "parameter_grid = (list(ParameterGrid(param_grid)))\n",
    "for parameter_combination in parameter_grid:\n",
    "    print('LEARNING RATE IS ' , parameter_combination.get('learning_rate') , 'BATCH SIZE IS' , parameter_combination.get('batch_size') )\n",
    "    sofmax_CNN = build_CNN_lr(parameter_combination.get('learning_rate'), x_train , y_train , x_test , y_test , 2 , parameter_combination.get('batch_size'))\n",
    "    del sofmax_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE IS  10 BATCH SIZE IS 128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 163704 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "163704/163704 [==============================] - 291s 2ms/step - loss: 1.6588 - acc: 0.5860 - val_loss: 1.1543 - val_acc: 0.6669\n",
      "Epoch 2/2\n",
      "163704/163704 [==============================] - 287s 2ms/step - loss: 1.3482 - acc: 0.6315 - val_loss: 1.0576 - val_acc: 0.6745\n",
      "LEARNING RATE IS  100 BATCH SIZE IS 128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 163704 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "163704/163704 [==============================] - 295s 2ms/step - loss: 7.0906 - acc: 0.5595 - val_loss: 6.9016 - val_acc: 0.5718\n",
      "Epoch 2/2\n",
      "163704/163704 [==============================] - 287s 2ms/step - loss: 7.0928 - acc: 0.5599 - val_loss: 6.9016 - val_acc: 0.5718\n",
      "LEARNING RATE IS  1000 BATCH SIZE IS 128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 163704 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "163704/163704 [==============================] - 295s 2ms/step - loss: 7.0905 - acc: 0.5595 - val_loss: 6.9016 - val_acc: 0.5718\n",
      "Epoch 2/2\n",
      "163704/163704 [==============================] - 304s 2ms/step - loss: 7.0925 - acc: 0.5600 - val_loss: 6.9016 - val_acc: 0.5718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {'learning_rate': [10 , 100 ,1000 ], 'batch_size': [128]}\n",
    "\n",
    "\n",
    "parameter_grid = (list(ParameterGrid(param_grid)))\n",
    "for parameter_combination in parameter_grid:\n",
    "    print('LEARNING RATE IS ' , parameter_combination.get('learning_rate') , 'BATCH SIZE IS' , parameter_combination.get('batch_size') )\n",
    "    sofmax_CNN = build_CNN_lr(parameter_combination.get('learning_rate'), x_train , y_train , x_test , y_test , 2 , parameter_combination.get('batch_size'))\n",
    "    del sofmax_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Which model do we choose according to the result and why ?</h5>\n",
    "\n",
    "First we split this part in two part : first determining the best batch size and then the learning rate.\n",
    "We try several batch size according to many examples we found on the internet, and we find it more interesting to use 128 because we gain time comparing to more \n",
    "\n",
    "In term of learning rate we tried from 0.01 to 1000 in the last two cells above. It shows us that we have to take 5 because if we take less we need to many epochs to train our model well but if we take more than 10 we don't manage to reach the minimun but just his neigbourhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2. Dropout <h4/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this we tried to implement a version with weight decay, which is a technique to implement a penalty on wieght to reduce overfitting. However in our case it doesn't look to be usefull as our val_acc decreases. Indeed, this is because we already have some dropout, but we will try in the next celle to change our dropout and to see what happen, to increase again our performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to build the CNN for different activation functions\n",
    "\n",
    "def build_CNN_lr_dropout(learning_rate ,dropout, x_train , y_train , x_test , y_test , epochs , batch_size):\n",
    "    CNN = Sequential()\n",
    "    CNN.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "    CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN.add(Conv2D(32, kernel_size=(3, 3), \n",
    "                     activation='relu'))\n",
    "    CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN.add(Conv2D(64, kernel_size=(3, 3), \n",
    "                     activation='relu'))\n",
    "    CNN.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    CNN.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    CNN.add(Dense(64, activation='relu'))\n",
    "    CNN.add(Dropout(dropout))\n",
    "    CNN.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    CNN.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr = learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    CNN.summary()\n",
    "\n",
    "    history = CNN.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    return(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dropout IS  0.5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 163704 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "163704/163704 [==============================] - 296s 2ms/step - loss: 1.6094 - acc: 0.5874 - val_loss: 1.0756 - val_acc: 0.6646\n",
      "Epoch 2/2\n",
      "163704/163704 [==============================] - 287s 2ms/step - loss: 1.2733 - acc: 0.6460 - val_loss: 1.0238 - val_acc: 0.6707\n",
      " Dropout IS  0.4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_34 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 163704 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "163704/163704 [==============================] - 292s 2ms/step - loss: 1.6121 - acc: 0.5853 - val_loss: 1.0540 - val_acc: 0.6732\n",
      "Epoch 2/2\n",
      "163704/163704 [==============================] - 287s 2ms/step - loss: 1.2614 - acc: 0.6470 - val_loss: 1.0045 - val_acc: 0.6809\n",
      " Dropout IS  0.3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 163704 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "163704/163704 [==============================] - 296s 2ms/step - loss: 1.5339 - acc: 0.5986 - val_loss: 1.0674 - val_acc: 0.6620\n",
      "Epoch 2/2\n",
      "163704/163704 [==============================] - 287s 2ms/step - loss: 1.2155 - acc: 0.6589 - val_loss: 0.8900 - val_acc: 0.7229\n",
      " Dropout IS  0.2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_40 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 163704 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "163704/163704 [==============================] - 296s 2ms/step - loss: 1.5064 - acc: 0.6001 - val_loss: 1.0650 - val_acc: 0.6725\n",
      "Epoch 2/2\n",
      "163704/163704 [==============================] - 293s 2ms/step - loss: 1.1700 - acc: 0.6679 - val_loss: 0.8625 - val_acc: 0.7278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {'dropout': [0.5 , 0.4 , 0.3 , 0.2 ]}\n",
    "\n",
    "\n",
    "parameter_grid = (list(ParameterGrid(param_grid)))\n",
    "for parameter_combination in parameter_grid:\n",
    "    print(' Dropout IS ' , parameter_combination.get('dropout') )\n",
    "    sofmax_CNN = build_CNN_lr_dropout(5 , parameter_combination.get('dropout'), x_train , y_train , x_test , y_test , 2 ,128 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>5. Model Evaluation</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some form of pre-evaluation will inevitably be required in the preceding sections in order to both select an appropriate model and configure its parameters appropriately.\n",
    "In this final section, you may evaluate other aspects of the model such as:\n",
    "\n",
    "- Assessing the running time of your model;\n",
    "- Determining whether some aspects can be parallelised;\n",
    "- Training the model with smaller subsets of the data.\n",
    "- etc.\n",
    "\n",
    "For the evaluation of the classification results, you should use the F1 measure (see Submission Instructions). Here the focus should be on level2 classification. A classification evaluation for other labels is optional.\n",
    "\n",
    "Please note that you are responsible for creating a sensible train/validation/test split. There is no predefined held-out test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS__:\n",
    "   \n",
    "For the train/test data we decided to choose the train_test_to_split from Sklearn with the stratify on our level2 to ensure that every level2 is represented proportionnaly compared to the full dataset.\n",
    "We did it on an 80% training, 20% testing to avoid overfitting by training on 90% of more. Our results are on the cells above \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS ON DISTRIBUTED PART:__\n",
    "\n",
    "So we see that our model demands a lot of computation time so we may think about a way to parralelize and distribute \n",
    "the computation across several machines (with Spark for instance) and the data across several machines with HDFS for instance.\n",
    "\n",
    "So if our data is big, let's simply use the HDFS storage system to split the data across th different machine. \n",
    "Here there is the case it can be usefull to avoid to load all the data on a single machine but ot split \n",
    "it across different machines\n",
    "\n",
    "No, let's think about how to distribute the computation work across several machines. One way to do it\n",
    "is to split our model in several machines, it can be split across multiple machines. For example, \n",
    "a single layer can be fit into the memory of a single machine and forward and backward propagation involves \n",
    "communication of output from one machine to another in a serial fashion. Furthermore, in our model which is\n",
    " a kind of combination of the same neural network but with different training sets we can just split the \n",
    "different training in the different machine. WIth this the computation time will reduce heavily. With this \n",
    "each worker will be in charge or training one model on a subset of the data and the final model will be the\n",
    " average of all (this is what i try to do but in a non distributed way) More generally, each machine can be \n",
    "responsible ofr training a certain part of the network, for instance each machine is responsible for training\n",
    " one layer but this is not quite effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> TRAINING ON THE FULL SET AND ASSESING THE RUNNING TIME <h4/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n"
     ]
    }
   ],
   "source": [
    "#THE AIM OF THIS CELL IS TO PERFORM IMAGE RESIZING AND INCREASE THE \n",
    "#DATASET SIZE FOR THE CATEGORIES WHICH ARE NOT WELL PRESENT IN THE DATAST\n",
    "\n",
    "img_files = extract_zip_to_memory(\"/mnt/datasets/plankton/flowcam/imgs.zip\")\n",
    "\n",
    "IMG_SIZE = 60\n",
    "\n",
    "j=0\n",
    "i=0\n",
    "img_arrays = {}\n",
    "\n",
    "\n",
    "# the data, split between train and test sets\n",
    "train_ids, test_ids, train_labels, test_labels = train_test_split(meta.objid, meta.level2, stratify = meta.level2, test_size=0.2, random_state=5)\n",
    "\n",
    "\n",
    "x_train, x_test = [] , []\n",
    "\n",
    " ###FOR EACH IMAGE OF THE TRAINING SET :  IE EACH LINE OF OUR DATAFRAME , WE RESIZE AND PERFORM DATA AUGMENTATION\n",
    "for img_id in train_ids:\n",
    "    \n",
    "    #Select the image, performs resizing and add it to our list of image in numpy\n",
    "    img = Image.open(img_files['imgs/' + str(int(img_id)) + '.jpg'])\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
    "    img = np.array(img)\n",
    "    #If this image is a part of the less representative class\n",
    "    if ( list(meta.loc[meta['objid'] == img_id, 'level2'])[0] in little_class): \n",
    "\n",
    "        #Getting the specie corresponding to this image to add a corresponding line to the meta\n",
    "        specie = list(meta.loc[meta['objid'] == img_id, 'level2'])[0]\n",
    "         \n",
    "        #Flipping left right\n",
    "        img_arrays[j]=  np.fliplr(img)\n",
    "        \n",
    "\n",
    "        #Flipping Up Down\n",
    "        img_arrays[j+1]=  np.flipud(img)\n",
    "\n",
    "\n",
    "        #Rotating two times with two different angle  \n",
    "        img_arrays[j+2]= scipy.ndimage.rotate(img, float(randint(-30,30)), reshape=False, order=5, mode='nearest')\n",
    "        \n",
    "        img_arrays[j+3]= scipy.ndimage.rotate(img, float(randint(-30,30)), reshape=False, order=5, mode='nearest')\n",
    "        \n",
    "\n",
    "        #Adding a three new lines in our df (one for each modification) with the unique_name corresponding to the original image\n",
    "        df = pd.DataFrame({'objid' :[j,j+1,j+2,j+3], 'level2' :[specie , specie ,specie ,specie]})\n",
    "        meta = meta.append(df, ignore_index=True)  \n",
    "        j+=4\n",
    "      \n",
    "    img = np.divide((img),255)\n",
    "    img = img.astype('float16')\n",
    "    x_train.append(img.reshape(IMG_SIZE,IMG_SIZE,1))\n",
    "    #To track how our program is advancing\n",
    "    i+=1\n",
    "    if (i%10000==0):\n",
    "        print(i)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDING THE NEW IMAGES TO THE TRAINING SET (ie adding it to training ids and label)\n",
    "for i in range (j):\n",
    "    train_ids= train_ids.append(pd.DataFrame([i]))\n",
    "    train_labels = train_labels.append(meta.level2.iloc[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERFORMING NORMALIZATION ON THE NEW IMAGES ADDED\n",
    "for i in range (j):\n",
    "    img = img_arrays[i].astype('float16')\n",
    "    x_train.append(img.reshape(IMG_SIZE,IMG_SIZE,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PERFORMING RESIZING AND NORMALIZATION ON THE TEST\n",
    "for img_id in test_ids:\n",
    "    #Select the image, performs resizing and add it to our list of image in numpy\n",
    "    img = Image.open(img_files['imgs/' + str(int(img_id)) + '.jpg'])\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
    "    img = np.array(img)\n",
    "    img  = np.divide(img,255)\n",
    "    img = img.astype('float16')\n",
    "    x_test.append(img.reshape(IMG_SIZE,IMG_SIZE,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (277757, 60, 60, 1)\n",
      "x_test shape: (48522, 60, 60, 1)\n",
      "y_train shape: (277757, 39)\n",
      "y_test shape: (48522, 39)\n"
     ]
    }
   ],
   "source": [
    "#For each id in train set add the image corresponding to the list\n",
    "#Same for the test set\n",
    "\n",
    "x_train, x_test = np.array(x_train), np.array(x_test)\n",
    "    \n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "num_classes = meta.level2.unique().size\n",
    "\n",
    "class_matrix = {}\n",
    "c = 0\n",
    "for label in meta.level2.unique():\n",
    "    class_matrix[label] = c\n",
    "    c+=1\n",
    "\n",
    "\n",
    "y_train, y_test = [], []\n",
    "for label in train_labels:\n",
    "    y_train.append(class_matrix[label])\n",
    "for label in test_labels:\n",
    "    y_test.append(class_matrix[label])\n",
    "    \n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 277757 samples, validate on 48522 samples\n",
      "Epoch 1/10\n",
      "277757/277757 [==============================] - 457s 2ms/step - loss: 3.2124 - acc: 0.6022 - val_loss: 0.9178 - val_acc: 0.7084\n",
      "Epoch 2/10\n",
      "277757/277757 [==============================] - 455s 2ms/step - loss: 2.9904 - acc: 0.6492 - val_loss: 0.8424 - val_acc: 0.7267\n",
      "Epoch 3/10\n",
      "277757/277757 [==============================] - 457s 2ms/step - loss: 2.9345 - acc: 0.6626 - val_loss: 0.7929 - val_acc: 0.7386\n",
      "Epoch 4/10\n",
      "277757/277757 [==============================] - 457s 2ms/step - loss: 2.9095 - acc: 0.6693 - val_loss: 0.7816 - val_acc: 0.7426\n",
      "Epoch 5/10\n",
      "277757/277757 [==============================] - 456s 2ms/step - loss: 2.8935 - acc: 0.6739 - val_loss: 0.7800 - val_acc: 0.7388\n",
      "Epoch 6/10\n",
      "277757/277757 [==============================] - 456s 2ms/step - loss: 2.8834 - acc: 0.6779 - val_loss: 0.7691 - val_acc: 0.7430\n",
      "Epoch 7/10\n",
      "277757/277757 [==============================] - 457s 2ms/step - loss: 2.8767 - acc: 0.6794 - val_loss: 0.7645 - val_acc: 0.7492\n",
      "Epoch 8/10\n",
      "277757/277757 [==============================] - 457s 2ms/step - loss: 2.8700 - acc: 0.6805 - val_loss: 0.7763 - val_acc: 0.7502\n",
      "Epoch 9/10\n",
      "277757/277757 [==============================] - 456s 2ms/step - loss: 2.8678 - acc: 0.6818 - val_loss: 0.7515 - val_acc: 0.7524\n",
      "Epoch 10/10\n",
      "277757/277757 [==============================] - 456s 2ms/step - loss: 2.8897 - acc: 0.6765 - val_loss: 0.8002 - val_acc: 0.7361\n",
      "4566.0447862148285\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time. time()\n",
    "sofmax_CNN = build_CNN_lr_dropout(5 , 0.2, x_train , y_train , x_test , y_test , 10 ,128 )\n",
    "end = time. time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMPUTATION TIME:__\n",
    "\n",
    "This model takes about 76 minutes to train what is very long. This can be explain by the number of different images e have and the data augmentation we have performed before.\n",
    "However, we train for about 10 epochs and we can see that our accuracy on validation data is about 0.75 and start to stay equals at about 7 epochs. That means that we can reduce our number of epoch.\n",
    "Furthermore, our accuracy on validation data is slighlty decreasing at about 10 epochs. THat means that we are overfitting, ie our model is well suited for the training data but not very much for the validation dta because it is becoming to specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#To test our function \n",
    "score=0\n",
    "#softmax_CNN = load_model('softmax_file.h5')\n",
    "y_pred = sofmax_CNN.predict(x_test)\n",
    "b = np.zeros_like(y_pred)\n",
    "b[np.arange(len(y_pred)), y_pred.argmax(1)] = 1\n",
    "\n",
    "\n",
    "\n",
    "score = f1_score(b , y_test , average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2804223586551583\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS ON FINAL F1 SCORE__ :\n",
    "\n",
    "As we can, although, we have good a good accuracy in general, ie we recognize more than 75% of the classes, our f1score with option= 'macro' isn't good at all. Indeed, this option doesn't take into account the wieght of each class (ie the number of the population for each class), but every class counts for 1. So we manage to recognize 28%  of the classes.\n",
    "\n",
    "This is due to the fact that our dataset is unbalanced. We've got so many detritus and for instance Bacteriastrum has only 12 representation in our data set. Even with the data augmentation it is still difficult for our CNN to learn the very few representative level2 without many epochs. This is why our f1 score isn't very good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> TRAINING ON THE DIFFERENT SIZE <h4/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE SIZE OF THE TRAINING SET IS: 0.8\n",
      "Started to preprocess\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "140000\n",
      "160000\n",
      "180000\n",
      "x_train shape: (277917, 60, 60, 1)\n",
      "x_test shape: (48522, 60, 60, 1)\n",
      "y_train shape: (277917, 39)\n",
      "y_test shape: (48522, 39)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 277917 samples, validate on 48522 samples\n",
      "Epoch 1/2\n",
      "277917/277917 [==============================] - 653s 2ms/step - loss: 1.6215 - acc: 0.5869 - val_loss: 1.1599 - val_acc: 0.6768\n",
      "Epoch 2/2\n",
      "277917/277917 [==============================] - 659s 2ms/step - loss: 1.4141 - acc: 0.6247 - val_loss: 1.0178 - val_acc: 0.6956\n",
      "48522/48522 [==============================] - 36s 748us/step\n",
      "THE SIZE OF THE TRAINING SET IS: 0.7\n",
      "Started to preprocess\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "140000\n",
      "160000\n",
      "x_train shape: (242852, 60, 60, 1)\n",
      "x_test shape: (72783, 60, 60, 1)\n",
      "y_train shape: (242852, 39)\n",
      "y_test shape: (72783, 39)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 242852 samples, validate on 72783 samples\n",
      "Epoch 1/2\n",
      "242852/242852 [==============================] - 593s 2ms/step - loss: 1.6382 - acc: 0.5856 - val_loss: 1.1734 - val_acc: 0.6612\n",
      "Epoch 2/2\n",
      "242852/242852 [==============================] - 591s 2ms/step - loss: 1.4003 - acc: 0.6278 - val_loss: 0.9658 - val_acc: 0.6974\n",
      "72783/72783 [==============================] - 54s 743us/step\n",
      "THE SIZE OF THE TRAINING SET IS: 0.6\n",
      "Started to preprocess\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "140000\n",
      "x_train shape: (207968, 60, 60, 1)\n",
      "x_test shape: (97043, 60, 60, 1)\n",
      "y_train shape: (207968, 39)\n",
      "y_test shape: (97043, 39)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 207968 samples, validate on 97043 samples\n",
      "Epoch 1/2\n",
      "207968/207968 [==============================] - 535s 3ms/step - loss: 1.6454 - acc: 0.5794 - val_loss: 1.0745 - val_acc: 0.6761\n",
      "Epoch 2/2\n",
      "207968/207968 [==============================] - 543s 3ms/step - loss: 1.4136 - acc: 0.6209 - val_loss: 1.0334 - val_acc: 0.6931\n",
      "97043/97043 [==============================] - 74s 758us/step\n",
      "THE SIZE OF THE TRAINING SET IS: 0.5\n",
      "Started to preprocess\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "100000\n",
      "120000\n",
      "x_train shape: (173199, 60, 60, 1)\n",
      "x_test shape: (121304, 60, 60, 1)\n",
      "y_train shape: (173199, 39)\n",
      "y_test shape: (121304, 39)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 173199 samples, validate on 121304 samples\n",
      "Epoch 1/2\n",
      "173199/173199 [==============================] - 416s 2ms/step - loss: 1.6898 - acc: 0.5693 - val_loss: 1.1867 - val_acc: 0.6444\n",
      "Epoch 2/2\n",
      "173199/173199 [==============================] - 322s 2ms/step - loss: 1.4513 - acc: 0.6097 - val_loss: 1.1234 - val_acc: 0.6387\n",
      "121304/121304 [==============================] - 60s 492us/step\n",
      "THE SIZE OF THE TRAINING SET IS: 0.4\n",
      "Started to preprocess\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "80000\n",
      "x_train shape: (138510, 60, 60, 1)\n",
      "x_test shape: (145565, 60, 60, 1)\n",
      "y_train shape: (138510, 39)\n",
      "y_test shape: (145565, 39)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 138510 samples, validate on 145565 samples\n",
      "Epoch 1/2\n",
      "138510/138510 [==============================] - 296s 2ms/step - loss: 1.7747 - acc: 0.5511 - val_loss: 1.2739 - val_acc: 0.6222\n",
      "Epoch 2/2\n",
      "138510/138510 [==============================] - 363s 3ms/step - loss: 1.5321 - acc: 0.5910 - val_loss: 1.1891 - val_acc: 0.6684\n",
      "145565/145565 [==============================] - 95s 653us/step\n",
      "THE SIZE OF THE TRAINING SET IS: 0.3\n",
      "Started to preprocess\n",
      "20000\n",
      "40000\n",
      "60000\n",
      "x_train shape: (103922, 60, 60, 1)\n",
      "x_test shape: (169825, 60, 60, 1)\n",
      "y_train shape: (103922, 39)\n",
      "y_test shape: (169825, 39)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 39)                2535      \n",
      "=================================================================\n",
      "Total params: 133,063\n",
      "Trainable params: 133,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 103922 samples, validate on 169825 samples\n",
      "Epoch 1/2\n",
      "103922/103922 [==============================] - 316s 3ms/step - loss: 1.8109 - acc: 0.5500 - val_loss: 1.3629 - val_acc: 0.6050\n",
      "Epoch 2/2\n",
      "103922/103922 [==============================] - 316s 3ms/step - loss: 1.5767 - acc: 0.5840 - val_loss: 1.1233 - val_acc: 0.6713\n",
      "169825/169825 [==============================] - 114s 670us/step\n"
     ]
    }
   ],
   "source": [
    "#THE AIM OF THIS CELL IS TO PERFORM IMAGE RESIZING AND INCREASE THE \n",
    "    #DATASET SIZE FOR THE CATEGORIES WHICH ARE NOT WELL PRESENT IN THE DATAST\n",
    "    \n",
    "size_list = [0.8 , 0.7 , 0.6 , 0.5 , 0.4 , 0.3 ]\n",
    "score_list = []\n",
    "img_files = extract_zip_to_memory(\"/mnt/datasets/plankton/flowcam/imgs.zip\")\n",
    "for size in size_list:\n",
    "    \n",
    "    meta = pd.read_csv('/mnt/datasets/plankton/flowcam/meta.csv')\n",
    "    \n",
    "    #Identifying the different level twos that are less present and we have to do data augmentatiin on it\n",
    "    different_species = meta.groupby(['level2'])['objid'].count()\n",
    "\n",
    "    \n",
    "    little_class = different_species.sort_values(ascending=True)[:30]\n",
    "\n",
    "    #Printing null value and droping it \n",
    "    null_columns=meta.columns[meta.isnull().any()]\n",
    " \n",
    "    meta =  meta.dropna(subset=['level2'])\n",
    "\n",
    "\n",
    "    little_class = list(little_class.index.sort_values(ascending=True)[:70])\n",
    "\n",
    "    #Dropping useless columns\n",
    "    meta = meta.drop(['longitude', 'objdate' ,'objtime' , 'depth_max' , 'depth_min' , 'level1', 'unique_name' , 'lineage', 'id' , 'projid', 'status', 'latitude'] ,axis=1)\n",
    "    print('THE SIZE OF THE TRAINING SET IS:' , size)\n",
    "\n",
    "    IMG_SIZE = 60\n",
    "\n",
    "    j=0\n",
    "    i=0\n",
    "    img_arrays = {}\n",
    "\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    train_ids, test_ids, train_labels, test_labels = train_test_split(meta.objid, meta.level2, test_size=1-size,stratify = meta.level2,random_state=5)\n",
    "\n",
    "\n",
    "    x_train, x_test = [] , []\n",
    "    print('Started to preprocess')\n",
    "     ###FOR EACH IMAGE OF THE TRAINING SET :  IE EACH LINE OF OUR DATAFRAME , WE RESIZE AND PERFORM DATA AUGMENTATION\n",
    "    for img_id in train_ids:\n",
    "\n",
    "        #Select the image, performs resizing and add it to our list of image in numpy\n",
    "        img = Image.open(img_files['imgs/' + str(int(img_id)) + '.jpg'])\n",
    "        img = img.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
    "        img = np.array(img)\n",
    "        #If this image is a part of the less representative class\n",
    "        if ( list(meta.loc[meta['objid'] == img_id, 'level2'])[0] in little_class): \n",
    "\n",
    "            #Getting the specie corresponding to this image to add a corresponding line to the meta\n",
    "            specie = list(meta.loc[meta['objid'] == img_id, 'level2'])[0]\n",
    "\n",
    "            #Flipping left right\n",
    "            img_arrays[j]=  np.fliplr(img)\n",
    "\n",
    "\n",
    "            #Flipping Up Down\n",
    "            img_arrays[j+1]=  np.flipud(img)\n",
    "\n",
    "\n",
    "            #Rotating two times with two different angle  \n",
    "            img_arrays[j+2]= scipy.ndimage.rotate(img, float(randint(-30,30)), reshape=False, order=5, mode='nearest')\n",
    "\n",
    "            img_arrays[j+3]= scipy.ndimage.rotate(img, float(randint(-30,30)), reshape=False, order=5, mode='nearest')\n",
    "\n",
    "\n",
    "            #Adding a three new lines in our df (one for each modification) with the unique_name corresponding to the original image\n",
    "            df = pd.DataFrame({'objid' :[j,j+1,j+2,j+3], 'level2' :[specie , specie ,specie ,specie]})\n",
    "            meta = meta.append(df, ignore_index=True)  \n",
    "            j+=4\n",
    "            \n",
    "        img = np.divide((img),255)\n",
    "        img = img.astype('float16')\n",
    "        x_train.append(img.reshape(IMG_SIZE,IMG_SIZE,1))\n",
    "        #To track how our program is advancing\n",
    "        i+=1\n",
    "        if (i%20000==0):\n",
    "            print(i)\n",
    "\n",
    "    #ADDING THE NEW IMAGES TO THE TRAINING SET (ie adding it to training ids and label)\n",
    "    for i in range (j):\n",
    "        train_ids= train_ids.append(pd.DataFrame([i]))\n",
    "        train_labels = train_labels.append(meta.level2.iloc[[i]])\n",
    "\n",
    "    #PERFORMING NORMALIZATION ON THE NEW IMAGES ADDED\n",
    "    for i in range (j):\n",
    "        img = np.array(img_arrays[i])\n",
    "        \n",
    "        img = np.divide((img),255)\n",
    "        img = img.astype('float16')\n",
    "        x_train.append(img.reshape(IMG_SIZE,IMG_SIZE,1))\n",
    "\n",
    "    ##PERFORMING RESIZING AND NORMALIZATION ON THE TEST\n",
    "    for img_id in test_ids:\n",
    "        #Select the image, performs resizing and add it to our list of image in numpy\n",
    "        img = Image.open(img_files['imgs/' + str(int(img_id)) + '.jpg'])\n",
    "        img = img.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
    "        img = np.array(img)\n",
    "        img = np.divide((img),255)\n",
    "        img = img.astype('float16')\n",
    "        x_test.append(img.reshape(IMG_SIZE,IMG_SIZE,1))\n",
    "\n",
    "\n",
    "    #For each id in train set add the image corresponding to the list\n",
    "    #Same for the test set\n",
    "\n",
    "    x_train, x_test = np.array(x_train), np.array(x_test)\n",
    "\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "    num_classes = meta.level2.unique().size\n",
    "\n",
    "    class_matrix = {}\n",
    "    c = 0\n",
    "    for label in meta.level2.unique():\n",
    "        class_matrix[label] = c\n",
    "        c+=1\n",
    "\n",
    "\n",
    "    y_train, y_test = [], []\n",
    "    for label in train_labels:\n",
    "        y_train.append(class_matrix[label])\n",
    "    for label in test_labels:\n",
    "        y_test.append(class_matrix[label])\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('y_test shape:', y_test.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    sofmax_CNN = build_CNN_lr_dropout(5 , 0.2, x_train , y_train , x_test , y_test , 2 ,128 )\n",
    "    score_list+=[sofmax_CNN.evaluate(x_test, y_test, verbose=1)[1]]\n",
    "    del img_arrays\n",
    "    del meta\n",
    "    del x_train\n",
    "    del x_test\n",
    "    \n",
    "    del sofmax_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Accuracy for different training sizes')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(size_list , score_list)\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for different training sizes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS:__\n",
    "\n",
    "In this part we can see that we have globally increasing performance (except for training size of 0.5 which must be a exception, like a bad initialisation of the weights). Indeed, our performance are not quite better with a bigger training size, at the end of the two epochs. This can be for one reason mainly. First, maybe as we've got ùany many detritus in our training set maybe we're just very likely to recognize detritus in with a less big data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b>N.B.</b> Please note that the items listed under each heading are neither exhaustive, nor are you expected to explore every given suggestion.\n",
    "    Nonetheless, these should serve as a guideline for your work in both this and upcoming challenges.\n",
    "    As always, you should use your intuition and understanding in order to decide which analysis best suits the assigned task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h2>Submission Instructions</h2>\n",
    "    <hr style=\"height:1px;border:none;color:#333;background-color:#333;\" />    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of this challenge is to construct a model for predicting Plankton (taxonomy level 2) classes.\n",
    "\n",
    "- Your submission will be the <b>HTML version of your notebook</b> exploring the various modelling aspects described above.\n",
    "\n",
    "- At the end of the notebook you should indicate your final evaluation score on a held-out test set. As an evaluation metric you should use the F1 score with the *average=macro* option as it is provided by the scikit-learn library. See the following link for more information:\n",
    "        \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2804223586551583\n"
     ]
    }
   ],
   "source": [
    "#FINAL SCORE WITH SKLEARN AVERAGE='MACRO'\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h2>Dataset Description</h2>\n",
    "    <hr style=\"height:1px;border:none;color:#333;background-color:#333;\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Location of the Dataset on zoe\n",
    "The data for this challenge is located at: `/mnt/datasets/plankton/flowcam`\n",
    "\n",
    "#### * Hierachical Taxonomy Tree for Labels \n",
    "\n",
    "Each object is represented by a single image and is identified by a unique integer number. It has a name associated to it which is integrated in a hierarchical taxonomic tree. The identifications are gathered from different projects, classified by different people in different contexts, so they often target different taxonomic levels. For example, let us say we classify items of clothing along the following tree\n",
    "\n",
    "    top\n",
    "        shirt\n",
    "            long sleeves\n",
    "            short sleeves\n",
    "        sweater\n",
    "            hooded\n",
    "            no hood\n",
    "    bottom\n",
    "        pants\n",
    "            jeans\n",
    "            other\n",
    "        shorts\n",
    "        \n",
    "In a first project, images are classified to the finest level possible, but it may be the case that, on some pictures, it is impossible to determine whether a sweater has a hood or not, in which case it is simply classified as `sweater`. In the second project, the operator classified tops as `shirt` or `sweater` only, and bottoms to the finest level. In a third project, the operator only separated tops from bottoms. In such a context, the original names in the database cannot be used directly because, for example `sweater` will contain images that are impossible to determine as `hooded` or `no hood` *as well as* `hooded` and `no hood` images that were simply not classified further. If all three classes (`sweater`, `hooded`, and `no hood`) are included in the training set, it will likely confuse the classifier. For this reason, we define different target taxonomic levels:\n",
    "\n",
    "-   `level1` is the finest taxonomic level possible. In the example above, we would include `hooded` and `no hood` but discard all images in `sweater` to avoid confusion; and proceed in the same manner for other classes.\n",
    "\n",
    "-   `level2` is a grouping of underlying levels. In the example above, it would include `shirt` (which contains all images in `shirt`, `long sleeves`, and `short sleeves`), `sweater` (which, similarly would include this class and all its children), `pants` (including children), and `shorts`. So typically, `level2` contains more images (less discarding), sorted within fewer classes than `level1`, and may therefore be an easier classification problem.\n",
    "\n",
    "-   `level3` is an even broader grouping. Here it would be `top` vs `bottom`\n",
    "\n",
    "-   etc.\n",
    "\n",
    "In the Plankton Image dataset, the objects will be categorised based on a pre-defined 'level1' and 'level2'. You can opt to work on one of them, but we recommend you to work on `level2` because it is an easier classification problem.  \n",
    "\n",
    "#### * Data Structure\n",
    "\n",
    "    /mnt/datasets/plankton/flowcam/\n",
    "        meta.csv\n",
    "        taxo.csv\n",
    "        features_native.csv.gz\n",
    "        features_skimage.csv.gz\n",
    "        imgs.zip\n",
    "\n",
    "* `meta.csv` contains the index of images and their corresponding labels\n",
    "* `taxo.csv` defines the taxonomic tree and its potential groupings at various level. Note that, the information is also available in `meta.csv`. Therefore, the information in `taxo.csv` is probably useless, but at least it gives you a global view about taxonomy tree\n",
    "* `features_native.csv.gz` contain the morphological handcrafted features computed by ZooProcess. In fact, ZooProcess generates the region of interests (ROI) around each individual object from a original image of Plankton. In addition, it also computes a set of associated features measured on the object. These features are the ones contained in `features_native.csv.gz`\n",
    "* `features_skimage.csv.gz` contains the morphological features recomputed with skimage.measure.regionprops on the ROIs produced by ZooProcess.\n",
    "* `imgs.zip` contains a post-processed version of the original images. Images are named by `objid`.jpg\n",
    "\n",
    "#### * Attributes in meta.csv\n",
    "\n",
    "The file contains the image identifiers (objid) as well as the labels assigned to the images by human operators. Those are defined with various levels of precision:\n",
    "\n",
    "* <i>unique_name</i>: raw labels from operators\n",
    "* <i>level1</i>: cleaned, most detailed labels\n",
    "* <i>level2</i>: regrouped (coarser) labels\n",
    "* <i>lineage</i>: full taxonomic lineage of the class\n",
    "\n",
    "Some labels may be missing (coded ‘NA’) at a given level, meaning that the corresponding objects should be discarded for the classification at this level.\n",
    "\n",
    "#### * imgs.zip\n",
    "\n",
    "This zip archive contains an *imgs* folder that contains all the images in .jpg format. Do not extract this folder to disk! Instead you will be loading the images to memory. See the code below for a quick how-to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Attributes in features_native.csv.gz\n",
    "A brief outline of the availabel attributes in `features_native.csv.gz` which you can use is given below:\n",
    "\n",
    "* <i>objid</i>: same as in `meta.csv`\n",
    "* <i>area</i>: area of ROI\n",
    "* <i>meanimagegrey</i>:\n",
    "* <i>mean</i>: mean grey\n",
    "* <i>stddev</i>: standard deviation of greys\n",
    "* <i>min</i>: minimum grey\n",
    "* <i>perim.</i>: perimeter of ROI\n",
    "* <i>width, height</i>: dimensions of ROI\n",
    "* <i>major, minor</i>: length of major,minor axis of the best fitting ellipse\n",
    "* <i>angle</i>: \n",
    "* <i>circ.</i>: circularity or shape factor which can be computed by 4pi(area/perim.^2)\n",
    "* <i>feret</i>:  maximal feret diameter\n",
    "* <i>intden</i>: integrated density: mean*area\n",
    "* <i>median</i>: median grey\n",
    "* <i>skew, kurt</i>: skewness,kurtosis of the histogram of greys\n",
    "* <i>%area</i>: proportion of the image corresponding to the object\n",
    "* <i>area_exc</i>: area excluding holes\n",
    "* <i>fractal</i>: fractal dimension of the perimeter\n",
    "* <i>skelarea</i>: area of the one-pixel wide skeleton of the image ???\n",
    "* <i>slope</i>: slope of the cumulated histogram of greys\n",
    "* <i>histcum1, 2, 3</i>:  grey level at quantiles 0.25, 0.5, 0.75 of the histogram of greys\n",
    "* <i>nb1, 2, 3</i>: number of objects after thresholding at the grey levels above\n",
    "* <i>symetrieh, symetriev</i>: index of horizontal,vertical symmetry\n",
    "* <i>symetriehc, symetrievc</i>: same but after thresholding at level histcum1\n",
    "* <i>convperim, convarea</i>: perimeter,area of the convex hull of the object\n",
    "* <i>fcons</i>: contrast\n",
    "* <i>thickr</i>: thickness ratio: maximum thickness/mean thickness\n",
    "* <i>esd</i>:\n",
    "* <i>elongation</i>: elongation index: major/minor\n",
    "* <i>range</i>: range of greys: max-min\n",
    "* <i>meanpos</i>:  relative position of the mean grey: (max-mean)/range\n",
    "* <i>centroids</i>:\n",
    "* <i>cv</i>: coefficient of variation of greys: 100*(stddev/mean)\n",
    "* <i>sr</i>: index of variation of greys: 100*(stddev/range)\n",
    "* <i>perimareaexc</i>:\n",
    "* <i>feretareaexc</i>:\n",
    "* <i>perimferet</i>: index of the relative complexity of the perimeter: perim/feret\n",
    "* <i>perimmajor</i>: index of the relative complexity of the perimeter: perim/major\n",
    "* <i>circex</i>:\n",
    "* <i>cdexc</i>:\n",
    "* <i>kurt_mean</i>:\n",
    "* <i>skew_mean</i>:\n",
    "* <i>convperim_perim</i>:\n",
    "* <i>convarea_area</i>:\n",
    "* <i>symetrieh_area</i>:\n",
    "* <i>symetriev_area</i>:\n",
    "* <i>nb1_area</i>:\n",
    "* <i>nb2_area</i>:\n",
    "* <i>nb3_area</i>:\n",
    "* <i>nb1_range</i>:\n",
    "* <i>nb2_range</i>:\n",
    "* <i>nb3_range</i>:\n",
    "* <i>median_mean</i>:\n",
    "* <i>median_mean_range</i>:\n",
    "* <i>skeleton_area</i>:\n",
    "\n",
    "#### * Attributes in features_skimage.csv.gz\n",
    "Table of morphological features recomputed with skimage.measure.regionprops on the ROIs produced by ZooProcess. See http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops for documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.irjet.net/archives/V4/i12/IRJET-V4I12123.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.sciencedirect.com/topics/engineering/image-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
