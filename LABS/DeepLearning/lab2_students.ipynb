{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning   </h1>\n",
    "<h1 style=\"text-align:center\"> Lab Session 2 - 1.5 Hours </h1>\n",
    "<h1 style=\"text-align:center\"> Convolutional Neural Network (CNN) for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this session is to practice with Convolutional Neural Networks. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "Follow instructions step by step until the end and submit your complete notebook as an archive (tar -cf groupXnotebook.tar DL_lab2/).\n",
    "\n",
    "Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by May 29th 2019 (23:59:59 CET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BERTELLI FLORIAN__\n",
    "\n",
    "__REBOULLET PAUL__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last Lab Session, you built a Multilayer Perceptron for recognizing hand-written digits from the MNIST data-set. The best achieved accuracy on testing data was about 97%.  Can  you do better than these results using a deep CNN ?\n",
    "In this Lab Session, you will build, train and optimize in TensorFlow one of the early Convolutional Neural Networks,  **LeNet-5**, to go to  more than 99% of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data in TensorFlow\n",
    "Run the cell below to load the MNIST data that comes with TensorFlow. You will use this data in **Section 1** and **Section 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-8854827bf5e0>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Image Shape: (784,)\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "from tensorflow.contrib.layers import flatten\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))\n",
    "\n",
    "epsilon = 1e-10 # this is a parameter you will use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : My First Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with CNN, let's train and test in TensorFlow the example\n",
    "**y=softmax(Wx+b)** seen in the first lab. \n",
    "\n",
    "This model reaches an accuracy of about 92 %.\n",
    "You will also learn how to launch the TensorBoard https://www.tensorflow.org/get_started/summaries_and_tensorboard to visualize the computation graph, statistics and learning curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : Read carefully the code in the cell below. Run it to perform training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  01   =====> Loss= 1.288298870\n",
      "Epoch:  02   =====> Loss= 0.732601715\n",
      "Epoch:  03   =====> Loss= 0.600517497\n",
      "Epoch:  04   =====> Loss= 0.536845536\n",
      "Epoch:  05   =====> Loss= 0.497668130\n",
      "Epoch:  06   =====> Loss= 0.471057313\n",
      "Epoch:  07   =====> Loss= 0.451252412\n",
      "Epoch:  08   =====> Loss= 0.436074483\n",
      "Epoch:  09   =====> Loss= 0.423697097\n",
      "Epoch:  10   =====> Loss= 0.413106918\n",
      "Epoch:  11   =====> Loss= 0.404530114\n",
      "Epoch:  12   =====> Loss= 0.396962555\n",
      "Epoch:  13   =====> Loss= 0.390427339\n",
      "Epoch:  14   =====> Loss= 0.384620576\n",
      "Epoch:  15   =====> Loss= 0.379247611\n",
      "Epoch:  16   =====> Loss= 0.374332983\n",
      "Epoch:  17   =====> Loss= 0.370459973\n",
      "Epoch:  18   =====> Loss= 0.366480190\n",
      "Epoch:  19   =====> Loss= 0.362761056\n",
      "Epoch:  20   =====> Loss= 0.359741725\n",
      "Epoch:  21   =====> Loss= 0.356469466\n",
      "Epoch:  22   =====> Loss= 0.354022222\n",
      "Epoch:  23   =====> Loss= 0.351207959\n",
      "Epoch:  24   =====> Loss= 0.348940905\n",
      "Epoch:  25   =====> Loss= 0.346488114\n",
      "Epoch:  26   =====> Loss= 0.344253443\n",
      "Epoch:  27   =====> Loss= 0.342160914\n",
      "Epoch:  28   =====> Loss= 0.340278962\n",
      "Epoch:  29   =====> Loss= 0.338371181\n",
      "Epoch:  30   =====> Loss= 0.336950148\n",
      "Epoch:  31   =====> Loss= 0.335198754\n",
      "Epoch:  32   =====> Loss= 0.333492791\n",
      "Epoch:  33   =====> Loss= 0.332004113\n",
      "Epoch:  34   =====> Loss= 0.330634121\n",
      "Epoch:  35   =====> Loss= 0.329121748\n",
      "Epoch:  36   =====> Loss= 0.327915255\n",
      "Epoch:  37   =====> Loss= 0.326601768\n",
      "Epoch:  38   =====> Loss= 0.325353343\n",
      "Epoch:  39   =====> Loss= 0.324200879\n",
      "Epoch:  40   =====> Loss= 0.322846569\n",
      "Optimization Finished!\n",
      "Accuracy: 0.916\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#STEP 1\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "logs_path = 'log_files/'  # useful for tensorboard\n",
    "\n",
    "# tf Graph Input:  mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition,  10 classes\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([10]), name='Bias')\n",
    "\n",
    "# Construct model and encapsulating all ops into scopes, making Tensorboard's Graph visualization more convenient\n",
    "with tf.name_scope('Model'):\n",
    "    # Model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "with tf.name_scope('Loss'):\n",
    "    # Minimize error using cross entropy\n",
    "    # We use tf.clip_by_value to avoid having too low numbers in the log function\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, epsilon, 1.0)), reduction_indices=1))\n",
    "with tf.name_scope('SGD'):\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"Loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"Accuracy\", acc)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#STEP 2 \n",
    "\n",
    "# Launch the graph for training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size, shuffle=(i==0))\n",
    "            # Run optimization op (backprop), cost op (to get loss value)\n",
    "            # and summary nodes\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                     feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch: \", '%02d' % (epoch+1), \"  =====> Loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    summary_writer.flush()\n",
    "\n",
    "    # Test model\n",
    "    # Calculate accuracy\n",
    "    print(\"Accuracy:\", acc.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2  </b>: Using Tensorboard, we can  now visualize the created graph, giving you an overview of your architecture and how all of the major components  are connected. You can also see and analyse the learning curves. \n",
    "\n",
    "To launch tensorBoard: \n",
    "- Open a Terminal and run the command line **\"tensorboard --logdir=lab_2/log_files/\"**\n",
    "- Click on \"Tensorboard web interface\" in Zoe  \n",
    "\n",
    "\n",
    "Enjoy It !! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : The 99% MNIST Challenge !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 1 </b> : LeNet5 implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now familar with **TensorFlow** and **TensorBoard**. In this section, you are to build, train and test the baseline [LeNet-5](http://yann.lecun.com/exdb/lenet/)  model for the MNIST digits recognition problem.  \n",
    "\n",
    "Then, you will make some optimizations to get more than 99% of accuracy.\n",
    "\n",
    "For more informations, have a look at this list of results: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"lenet.png\" width=\"800\" height=\"600\" align=\"center\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The LeNet architecture takes a 28x28xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "--------------------------\n",
    "**Layer 1 - Convolution (5x5):** The output shape should be 28x28x6. **Activation:** ReLU. **MaxPooling:** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2 - Convolution (5x5):** The output shape should be 10x10x16. **Activation:** ReLU. **MaxPooling:** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten:** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.  You may need to use tf.reshape.\n",
    "\n",
    "**Layer 3 - Fully Connected:** This should have 120 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 4 - Fully Connected:** This should have 84 outputs. **Activation:** ReLU.\n",
    "\n",
    "**Layer 5 - Fully Connected:** This should have 10 outputs. **Activation:** softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.1 </b>  Implement the Neural Network architecture described above.\n",
    "For that, your will use classes and functions from  https://www.tensorflow.org/api_docs/python/tf/nn. \n",
    "\n",
    "We give you some helper functions for weigths and bias initilization. Also you can refer to section 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for weigths and bias initilization \n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0., shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model(image): \n",
    "    \n",
    "    #Defining the firdt convolutional layer\n",
    "    conv1 = tf.nn.conv2d(\n",
    "        input=image,\n",
    "        filter=weight_variable((5, 5, 1, 6)), \n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='SAME',\n",
    "        name='conv1'\n",
    "    )\n",
    "    \n",
    "    #Adding the activation function and the pooling layer\n",
    "    conv1 = tf.nn.relu(conv1 + bias_variable([1, 28, 28, 6]))\n",
    "    max_pool1 = tf.nn.max_pool(\n",
    "        value=conv1,\n",
    "        ksize=[1,2,2,1],\n",
    "        strides=[1,2,2,1], #H2=(H1-K)/S +1 (output shape 1x14x14x6)\n",
    "        padding='VALID',\n",
    "        name='max_pool1'\n",
    "    )\n",
    "    \n",
    "    #Defining the second convolutional layer\n",
    "    conv2 = tf.nn.conv2d(\n",
    "        input=max_pool1,\n",
    "        filter=weight_variable((5, 5, 6, 16)), \n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name='conv2'\n",
    "    )\n",
    "    \n",
    "    #Adding the activation function and the pooling layer\n",
    "    conv2 = tf.nn.relu(conv2 + bias_variable([1, 10, 10, 16]))\n",
    "    max_pool2 = tf.nn.max_pool(\n",
    "        value=conv2,\n",
    "        ksize=[1,2,2,1],\n",
    "        strides=[1,2,2,1], #H2=(H1-K)/S +1 (output shape 1x5x5x16)\n",
    "        padding='VALID',\n",
    "        name='max_pool2'\n",
    "    )\n",
    "    \n",
    "    #Flatten the layer\n",
    "    flat = flatten(max_pool2)\n",
    " \n",
    "    \n",
    "    # Fully Connected with 120 neurons with ReLu\n",
    "    dense1 = tf.layers.dense(\n",
    "        inputs=flat,\n",
    "        units=120,\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.constant_initializer(value=1.0),\n",
    "        name='dense11'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Fully Connected with 84 neurons with ReLu\n",
    "    dense2 = tf.layers.dense(\n",
    "        inputs=dense1,\n",
    "        units=84,\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.constant_initializer(value=1.0),\n",
    "        name='dense21'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Fully Connected with 10 neurons with SoftMax\n",
    "    dense3 = tf.layers.dense(\n",
    "        inputs=dense2,\n",
    "        units=10,\n",
    "        activation=tf.nn.softmax,\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.constant_initializer(value=1.0),\n",
    "        name='dense31'\n",
    "    )\n",
    "    \n",
    "    # Return predictions\n",
    "    return dense3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.2. </b>  Calculate the number of parameters of this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation we have :\n",
    "\n",
    "    - conv 1  + bias :  5*5*1 + 6\n",
    "    - conv 2  + bias : 5*5*6 + 16\n",
    "    - dense 1 + bias : 5*5*16*120 + 120\n",
    "    - dense 2 + bias : 84*120 + 84\n",
    "    - dense 3 + bias : 10*84 +10\n",
    "    \n",
    "  \n",
    "Indeed, for each layer we have to sum the nummber of weight and the bias we had.\n",
    "    \n",
    "    \n",
    "The total of parameters is 59331"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.3. </b>  Define your model, its accuracy and the loss function according to the following parameters (you can look at Section 1 to see what is expected):\n",
    "\n",
    "     Learning rate: 0.001\n",
    "     Loss Fucntion: Cross-entropy\n",
    "     Optimizer: tf.train.GradientDescentOptimizer\n",
    "     Number of epochs: 40\n",
    "     Batch size: 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # reset the default graph before defining a new model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 40\n",
    "batch_size = 128\n",
    "logs_path = 'log_files_/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.4. </b>  Implement the evaluation function for accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, y):\n",
    "    correct = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.5. </b>  Implement training pipeline and run the training data through it to train the model.\n",
    "\n",
    "- Before each epoch, shuffle the training set. \n",
    "- Print the loss per mini batch and the training/validation accuracy per epoch. (Display results every 100 epochs)\n",
    "- Save the model after training\n",
    "- Print after training the final testing accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(learning_rate, training_epochs, batch_size, display_step = 1, \\\n",
    "          logs_path='log_files_/', optimizerF=\"SGD\", verbose=True , transfer='sigmoid'):\n",
    "    start = time.time()\n",
    "    optimizer_dict = {\"SGD\":tf.train.GradientDescentOptimizer, \"Adam\":tf.train.AdamOptimizer}\n",
    "    \n",
    "    # Erase previous graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='InputData')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "\n",
    "    # Construct model\n",
    "    with tf.name_scope('Model'):\n",
    "        pred = LeNet5_Model(x)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    with tf.name_scope('Loss'):\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "    with tf.name_scope(optimizerF):\n",
    "        if transfer is \"sigmoid\":\n",
    "            optimizer = optimizer_dict[optimizerF](learning_rate).minimize(cost)\n",
    "        else:\n",
    "            opt = optimizer_dict[optimizerF](learning_rate)\n",
    "            gvs = opt.compute_gradients(cost)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "            optimizer = opt.apply_gradients(capped_gvs)\n",
    "\n",
    "    # Evaluate model\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = evaluate(pred, y)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    x_val, y_val = mnist.validation.images.reshape(-1, 28, 28, 1), mnist.validation.labels\n",
    "    x_test, y_test = mnist.test.images.reshape(-1, 28, 28, 1), mnist.test.labels\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "#         acc_history = []\n",
    "        test_history = []\n",
    "        val_history = []\n",
    "#         train_history = []\n",
    "\n",
    "        sess.run(init)\n",
    "        if verbose is True:\n",
    "            print(\"Start Training!\")\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "        #Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            #Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                         feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "#             train_acc = accuracy.eval({x: batch_xs, y:batch_ys})\n",
    "            val_acc = accuracy.eval({x: x_val, y:y_val})\n",
    "            test_acc = accuracy.eval({x: x_test, y:y_test})\n",
    "#             acc_history.append(acc)\n",
    "#             train_history.append(train_acc)\n",
    "            val_history.append(val_acc)\n",
    "            test_history.append(test_acc)\n",
    "            \n",
    "            saver.save(sess, 'Models/model_' + str(learning_rate) + '_' + str(batch_size) + '_' + optimizerF)\n",
    "            if verbose is True and (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \\\n",
    "                      \"  =====> Loss=\", \"{:.9f}\".format(avg_cost), \\\n",
    "                      \" Validation accuracy=\", val_acc, \" Test accuracy=\", test_acc)\n",
    "            if val_acc>=0.99:\n",
    "                if verbose is True:\n",
    "                    print(\"Validation Accuracy over 99%% reached after %d epochs\" %(epoch+1))\n",
    "                break\n",
    "        print('It takes ' , time.time()-start , 'to train')  \n",
    "        start = time.time()\n",
    "        if verbose is True:\n",
    "            print(\"Training Finished!\")\n",
    "            # Test model\n",
    "            # Calculate accuracy\n",
    "            print(\"Test accuracy:\", accuracy.eval({x: x_test, y:y_test}))\n",
    "        print('It takes ' , time.time()-start , 'to test')  \n",
    "    return val_history, test_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 2.359788304  Validation accuracy= 0.3548  Test accuracy= 0.345\n",
      "Epoch:  02   =====> Loss= 1.984217592  Validation accuracy= 0.5362  Test accuracy= 0.5385\n",
      "Epoch:  03   =====> Loss= 1.494585602  Validation accuracy= 0.7168  Test accuracy= 0.7205\n",
      "Epoch:  04   =====> Loss= 0.966587138  Validation accuracy= 0.8036  Test accuracy= 0.806\n",
      "Epoch:  05   =====> Loss= 0.663597779  Validation accuracy= 0.8454  Test accuracy= 0.8508\n",
      "Epoch:  06   =====> Loss= 0.523834231  Validation accuracy= 0.87  Test accuracy= 0.8733\n",
      "Epoch:  07   =====> Loss= 0.449842068  Validation accuracy= 0.8832  Test accuracy= 0.887\n",
      "Epoch:  08   =====> Loss= 0.403340158  Validation accuracy= 0.892  Test accuracy= 0.8951\n",
      "Epoch:  09   =====> Loss= 0.370185190  Validation accuracy= 0.9006  Test accuracy= 0.8994\n",
      "Epoch:  10   =====> Loss= 0.346036175  Validation accuracy= 0.9048  Test accuracy= 0.9077\n",
      "Epoch:  11   =====> Loss= 0.325403106  Validation accuracy= 0.9092  Test accuracy= 0.9127\n",
      "Epoch:  12   =====> Loss= 0.308813778  Validation accuracy= 0.9148  Test accuracy= 0.9163\n",
      "Epoch:  13   =====> Loss= 0.293658464  Validation accuracy= 0.9206  Test accuracy= 0.9199\n",
      "Epoch:  14   =====> Loss= 0.282217285  Validation accuracy= 0.9226  Test accuracy= 0.9241\n",
      "Epoch:  15   =====> Loss= 0.270564046  Validation accuracy= 0.9244  Test accuracy= 0.9265\n",
      "Epoch:  16   =====> Loss= 0.258787728  Validation accuracy= 0.9278  Test accuracy= 0.9286\n",
      "Epoch:  17   =====> Loss= 0.252384093  Validation accuracy= 0.9294  Test accuracy= 0.9309\n",
      "Epoch:  18   =====> Loss= 0.240203086  Validation accuracy= 0.9318  Test accuracy= 0.9341\n",
      "Epoch:  19   =====> Loss= 0.234706766  Validation accuracy= 0.9346  Test accuracy= 0.9356\n",
      "Epoch:  20   =====> Loss= 0.225131066  Validation accuracy= 0.9374  Test accuracy= 0.9381\n",
      "Epoch:  21   =====> Loss= 0.219335954  Validation accuracy= 0.9386  Test accuracy= 0.9403\n",
      "Epoch:  22   =====> Loss= 0.213267121  Validation accuracy= 0.9408  Test accuracy= 0.9425\n",
      "Epoch:  23   =====> Loss= 0.205297484  Validation accuracy= 0.9418  Test accuracy= 0.9425\n",
      "Epoch:  24   =====> Loss= 0.201738516  Validation accuracy= 0.945  Test accuracy= 0.9451\n",
      "Epoch:  25   =====> Loss= 0.193175187  Validation accuracy= 0.948  Test accuracy= 0.9468\n",
      "Epoch:  26   =====> Loss= 0.190420875  Validation accuracy= 0.9492  Test accuracy= 0.9489\n",
      "Epoch:  27   =====> Loss= 0.184985832  Validation accuracy= 0.9494  Test accuracy= 0.9509\n",
      "Epoch:  28   =====> Loss= 0.178105833  Validation accuracy= 0.9512  Test accuracy= 0.9512\n",
      "Epoch:  29   =====> Loss= 0.175850529  Validation accuracy= 0.9524  Test accuracy= 0.9539\n",
      "Epoch:  30   =====> Loss= 0.170587886  Validation accuracy= 0.9536  Test accuracy= 0.9533\n",
      "Epoch:  31   =====> Loss= 0.166638493  Validation accuracy= 0.9554  Test accuracy= 0.955\n",
      "Epoch:  32   =====> Loss= 0.161236931  Validation accuracy= 0.9564  Test accuracy= 0.956\n",
      "Epoch:  33   =====> Loss= 0.160865457  Validation accuracy= 0.9574  Test accuracy= 0.9563\n",
      "Epoch:  34   =====> Loss= 0.155532101  Validation accuracy= 0.9582  Test accuracy= 0.9582\n",
      "Epoch:  35   =====> Loss= 0.151097578  Validation accuracy= 0.9592  Test accuracy= 0.96\n",
      "Epoch:  36   =====> Loss= 0.149547605  Validation accuracy= 0.9596  Test accuracy= 0.9587\n",
      "Epoch:  37   =====> Loss= 0.144410890  Validation accuracy= 0.9612  Test accuracy= 0.9592\n",
      "Epoch:  38   =====> Loss= 0.143527258  Validation accuracy= 0.9612  Test accuracy= 0.9624\n",
      "Epoch:  39   =====> Loss= 0.139303899  Validation accuracy= 0.9624  Test accuracy= 0.9631\n",
      "Epoch:  40   =====> Loss= 0.136482224  Validation accuracy= 0.964  Test accuracy= 0.9611\n",
      "It takes  626.8330988883972 to train\n",
      "Training Finished!\n",
      "Test accuracy: 0.9611\n",
      "It takes  1.081338882446289 to test\n"
     ]
    }
   ],
   "source": [
    "# trainining with first configuration\n",
    "val_hist, test_hist = train(learning_rate, training_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1.6 </b> : Use TensorBoard to visualise and save loss and accuracy curves. \n",
    "You will save figures in the folder **\"lab_2/MNIST_figures\"** and display them in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./MNIST_figures/TF_Data_WithOut_Dropout.png\" width=\"600\" height=\"400\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS:__\n",
    "\n",
    "As we can see the accuracy is growing as we follow the gradient in the decreasing way. In opposite, our loss is decreasing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Part 2 </b> : LeNET 5 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b> Question 2.2.1 </b>\n",
    "\n",
    "- Retrain your network with AdamOptimizer and then fill the table above:\n",
    "\n",
    "\n",
    "| Optimizer            |  Gradient Descent  |    AdamOptimizer    |\n",
    "|----------------------|--------------------|---------------------|\n",
    "| Testing Accuracy     |      0.958       |      0.992          |       \n",
    "| Training Time (second)       |         703        |        162          |  \n",
    "\n",
    "- Which optimizer gives the best accuracy on test data?\n",
    "\n",
    "**Your answer:** \n",
    "\n",
    "The optimizer which gives the best accuracy on the Test Data is the AdamOptimizer (0.992 vs 0.9655) for a shorter training time too.\n",
    "Indeed, compared to the Gradient Descent Optimizer, Adam Optimizer instead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance).\n",
    "\n",
    "Specifically, the algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages.\n",
    "\n",
    "This leads to have a moving learning rate constructed to reach the global minimum. Gradient Descent's learning rate doesn't move at all and so it is not very tuned for complicated minimun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 0.341464647  Validation accuracy= 0.9686  Test accuracy= 0.9702\n",
      "Epoch:  02   =====> Loss= 0.073869712  Validation accuracy= 0.9808  Test accuracy= 0.9788\n",
      "Epoch:  03   =====> Loss= 0.052109897  Validation accuracy= 0.983  Test accuracy= 0.983\n",
      "Epoch:  04   =====> Loss= 0.039654047  Validation accuracy= 0.9876  Test accuracy= 0.9869\n",
      "Epoch:  05   =====> Loss= 0.033007712  Validation accuracy= 0.9884  Test accuracy= 0.9886\n",
      "Epoch:  06   =====> Loss= 0.026240505  Validation accuracy= 0.9892  Test accuracy= 0.9894\n",
      "Epoch:  07   =====> Loss= 0.020087567  Validation accuracy= 0.9892  Test accuracy= 0.988\n",
      "Epoch:  08   =====> Loss= 0.018770608  Validation accuracy= 0.989  Test accuracy= 0.987\n",
      "Epoch:  09   =====> Loss= 0.016544443  Validation accuracy= 0.9882  Test accuracy= 0.9884\n",
      "Epoch:  10   =====> Loss= 0.012987009  Validation accuracy= 0.9908  Test accuracy= 0.9905\n",
      "Validation Accuracy over 99% reached after 10 epochs\n",
      "It takes  156.5672001838684 to train\n",
      "Training Finished!\n",
      "Test accuracy: 0.9905\n",
      "It takes  1.0362091064453125 to test\n"
     ]
    }
   ],
   "source": [
    "# trainining with first configuration\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# your implementation goes here\n",
    "\n",
    "val_hist, test_hist = train(learning_rate, training_epochs, batch_size,optimizerF='Adam' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./MNIST_figures/TF_Data_Adam.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__COMMENTS:__\n",
    "\n",
    "As we can see the curves with adam optimizer increase faster (for the accuracy) and the loss decreases faster too compared to the one with a fixed learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.2.2</b> Try to add dropout (keep_prob = 0.75) before the first fully connected layer. You will use tf.nn.dropout for that purpose. What accuracy do you achieve on testing data?\n",
    "\n",
    "**Accuracy achieved on testing data:** \n",
    "\n",
    "- Whitout Adam :\n",
    "\n",
    "It takes the same time to train (about 626 seconds) but the accuracy is less good than without dropout (0.946 vs 0.9611)\n",
    "\n",
    "- With Adam:\n",
    "\n",
    "It takes more time to train (about 340 vs 80 seconds) and the accuracy is the same as the end.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5_Model_Dropout(image , probability):    \n",
    "    conv1 = tf.nn.conv2d(\n",
    "        input=image,\n",
    "        filter=weight_variable((5, 5, 1, 6)), \n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='SAME',\n",
    "        name='conv1'\n",
    "    )\n",
    "    \n",
    "    conv1 = tf.nn.relu(conv1 + bias_variable([1, 28, 28, 6]))\n",
    "\n",
    "\n",
    "    max_pool1 = tf.nn.max_pool(\n",
    "        value=conv1,\n",
    "        ksize=[1,2,2,1],\n",
    "        strides=[1,2,2,1], #H2=(H1-K)/S +1 (output shape 1x14x14x6)\n",
    "        padding='VALID',\n",
    "        name='max_pool1'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    conv2 = tf.nn.conv2d(\n",
    "        input=max_pool1,\n",
    "        filter=weight_variable((5, 5, 6, 16)), \n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name='conv2'\n",
    "    )\n",
    "\n",
    "    \n",
    "    conv2 = tf.nn.relu(conv2 + bias_variable([1, 10, 10, 16]))\n",
    " \n",
    "        \n",
    "    max_pool2 = tf.nn.max_pool(\n",
    "        value=conv2,\n",
    "        ksize=[1,2,2,1],\n",
    "        strides=[1,2,2,1], #H2=(H1-K)/S +1 (output shape 1x5x5x16)\n",
    "        padding='VALID',\n",
    "        name='max_pool2'\n",
    "    )\n",
    "   \n",
    "    \n",
    "    flat = flatten(max_pool2)\n",
    " \n",
    "    #Adding dropout before the first dense layer\n",
    "    dropout_layer = tf.nn.dropout(\n",
    "    flat,\n",
    "    keep_prob=probability)\n",
    "    \n",
    "    # Fully Connected with 120 neurons\n",
    "    dense1 = tf.layers.dense(\n",
    "        inputs=dropout_layer,\n",
    "        units=120,\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.constant_initializer(value=1.0),\n",
    "        name='dense11'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Fully Connected with 84 neurons\n",
    "    dense2 = tf.layers.dense(\n",
    "        inputs=dense1,\n",
    "        units=84,\n",
    "        activation=tf.nn.relu,\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.constant_initializer(value=1.0),\n",
    "        name='dense21'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Fully Connected with 10 neurons\n",
    "    dense3 = tf.layers.dense(\n",
    "        inputs=dense2,\n",
    "        units=10,\n",
    "        activation=tf.nn.softmax,\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.constant_initializer(value=1.0),\n",
    "        name='dense31'\n",
    "    )\n",
    "    \n",
    "    # Return predictions\n",
    "    return dense3\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# your implementation goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_DropOut(probability, learning_rate, training_epochs, batch_size, display_step = 1, \\\n",
    "          logs_path='log_files_DropOut/', optimizerF=\"SGD\", verbose=True , transfer='sigmoid'):\n",
    "    start = time.time()\n",
    "    optimizer_dict = {\"SGD\":tf.train.GradientDescentOptimizer, \"Adam\":tf.train.AdamOptimizer}\n",
    "    \n",
    "    # Erase previous graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28, 1], name='InputData')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "\n",
    "    # Construct model\n",
    "    with tf.name_scope('Model'):\n",
    "        pred = LeNet5_Model_Dropout(x , probability)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    with tf.name_scope('Loss'):\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "    with tf.name_scope(optimizerF):\n",
    "        if transfer is \"sigmoid\":\n",
    "            optimizer = optimizer_dict[optimizerF](learning_rate).minimize(cost)\n",
    "        else:\n",
    "            opt = optimizer_dict[optimizerF](learning_rate)\n",
    "            gvs = opt.compute_gradients(cost)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "            optimizer = opt.apply_gradients(capped_gvs)\n",
    "\n",
    "    # Evaluate model\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = evaluate(pred, y)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a summary to monitor cost tensor\n",
    "    tf.summary.scalar(\"Loss\", cost)\n",
    "    # Create a summary to monitor accuracy tensor\n",
    "    tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    x_val, y_val = mnist.validation.images.reshape(-1, 28, 28, 1), mnist.validation.labels\n",
    "    x_test, y_test = mnist.test.images.reshape(-1, 28, 28, 1), mnist.test.labels\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "#         acc_history = []\n",
    "        test_history = []\n",
    "        val_history = []\n",
    "#         train_history = []\n",
    "\n",
    "        sess.run(init)\n",
    "        if verbose is True:\n",
    "            print(\"Start Training!\")\n",
    "        # op to write logs to Tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "        #Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            #Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "                batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "                # Run optimization op (backprop), cost op (to get loss value)\n",
    "                # and summary nodes\n",
    "                _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                                         feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "#             train_acc = accuracy.eval({x: batch_xs, y:batch_ys})\n",
    "            val_acc = accuracy.eval({x: x_val, y:y_val})\n",
    "            test_acc = accuracy.eval({x: x_test, y:y_test})\n",
    "#             acc_history.append(acc)\n",
    "#             train_history.append(train_acc)\n",
    "            val_history.append(val_acc)\n",
    "            test_history.append(test_acc)\n",
    "            \n",
    "            saver.save(sess, 'Models/model_' + str(learning_rate) + '_' + str(batch_size) + '_' + optimizerF)\n",
    "            if verbose is True and (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch: \", '%02d' % (epoch+1), \\\n",
    "                      \"  =====> Loss=\", \"{:.9f}\".format(avg_cost), \\\n",
    "                      \" Validation accuracy=\", val_acc, \" Test accuracy=\", test_acc)\n",
    "            if val_acc>=0.99:\n",
    "                if verbose is True:\n",
    "                    print(\"Validation Accuracy over 99%% reached after %d epochs\" %(epoch+1))\n",
    "                break\n",
    "        print('It takes ' , time.time()-start , 'to train')  \n",
    "        start = time.time()\n",
    "        if verbose is True:\n",
    "            print(\"Training Finished!\")\n",
    "            # Test model\n",
    "            # Calculate accuracy\n",
    "            print(\"Test accuracy:\", accuracy.eval({x: x_test, y:y_test}))\n",
    "        print('It takes ' , time.time()-start , 'to test')  \n",
    "    return val_history, test_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 2.355519645  Validation accuracy= 0.2622  Test accuracy= 0.2526\n",
      "Epoch:  02   =====> Loss= 2.065052887  Validation accuracy= 0.4762  Test accuracy= 0.4774\n",
      "Epoch:  03   =====> Loss= 1.630875981  Validation accuracy= 0.6156  Test accuracy= 0.6257\n",
      "Epoch:  04   =====> Loss= 1.179595354  Validation accuracy= 0.7122  Test accuracy= 0.7129\n",
      "Epoch:  05   =====> Loss= 0.896893010  Validation accuracy= 0.7656  Test accuracy= 0.7733\n",
      "Epoch:  06   =====> Loss= 0.738787021  Validation accuracy= 0.8038  Test accuracy= 0.8117\n",
      "Epoch:  07   =====> Loss= 0.634889965  Validation accuracy= 0.8212  Test accuracy= 0.832\n",
      "Epoch:  08   =====> Loss= 0.562248279  Validation accuracy= 0.8466  Test accuracy= 0.8501\n",
      "Epoch:  09   =====> Loss= 0.513525600  Validation accuracy= 0.8588  Test accuracy= 0.8606\n",
      "Epoch:  10   =====> Loss= 0.473145507  Validation accuracy= 0.869  Test accuracy= 0.881\n",
      "Epoch:  11   =====> Loss= 0.435199199  Validation accuracy= 0.8752  Test accuracy= 0.8849\n",
      "Epoch:  12   =====> Loss= 0.417038630  Validation accuracy= 0.8856  Test accuracy= 0.8932\n",
      "Epoch:  13   =====> Loss= 0.389253586  Validation accuracy= 0.8948  Test accuracy= 0.8961\n",
      "Epoch:  14   =====> Loss= 0.369768580  Validation accuracy= 0.8978  Test accuracy= 0.8995\n",
      "Epoch:  15   =====> Loss= 0.352495900  Validation accuracy= 0.8964  Test accuracy= 0.9071\n",
      "Epoch:  16   =====> Loss= 0.333286099  Validation accuracy= 0.9048  Test accuracy= 0.9131\n",
      "Epoch:  17   =====> Loss= 0.323991595  Validation accuracy= 0.9102  Test accuracy= 0.9163\n",
      "Epoch:  18   =====> Loss= 0.310774449  Validation accuracy= 0.912  Test accuracy= 0.9178\n",
      "Epoch:  19   =====> Loss= 0.300963255  Validation accuracy= 0.921  Test accuracy= 0.9139\n",
      "Epoch:  20   =====> Loss= 0.291115973  Validation accuracy= 0.92  Test accuracy= 0.9227\n",
      "Epoch:  21   =====> Loss= 0.283188911  Validation accuracy= 0.9188  Test accuracy= 0.9259\n",
      "Epoch:  22   =====> Loss= 0.273526004  Validation accuracy= 0.9266  Test accuracy= 0.9238\n",
      "Epoch:  23   =====> Loss= 0.264266249  Validation accuracy= 0.9268  Test accuracy= 0.9262\n",
      "Epoch:  24   =====> Loss= 0.260617359  Validation accuracy= 0.9254  Test accuracy= 0.9318\n",
      "Epoch:  25   =====> Loss= 0.253345534  Validation accuracy= 0.926  Test accuracy= 0.9326\n",
      "Epoch:  26   =====> Loss= 0.247110448  Validation accuracy= 0.9346  Test accuracy= 0.9332\n",
      "Epoch:  27   =====> Loss= 0.242728227  Validation accuracy= 0.9374  Test accuracy= 0.9351\n",
      "Epoch:  28   =====> Loss= 0.235678175  Validation accuracy= 0.9348  Test accuracy= 0.9372\n",
      "Epoch:  29   =====> Loss= 0.231383753  Validation accuracy= 0.9348  Test accuracy= 0.9381\n",
      "Epoch:  30   =====> Loss= 0.227450559  Validation accuracy= 0.9398  Test accuracy= 0.9368\n",
      "Epoch:  31   =====> Loss= 0.221156751  Validation accuracy= 0.94  Test accuracy= 0.9397\n",
      "Epoch:  32   =====> Loss= 0.214817808  Validation accuracy= 0.9364  Test accuracy= 0.9379\n",
      "Epoch:  33   =====> Loss= 0.214565661  Validation accuracy= 0.9418  Test accuracy= 0.9413\n",
      "Epoch:  34   =====> Loss= 0.209771820  Validation accuracy= 0.94  Test accuracy= 0.942\n",
      "Epoch:  35   =====> Loss= 0.205597796  Validation accuracy= 0.9426  Test accuracy= 0.9439\n",
      "Epoch:  36   =====> Loss= 0.204445354  Validation accuracy= 0.9428  Test accuracy= 0.9445\n",
      "Epoch:  37   =====> Loss= 0.197474242  Validation accuracy= 0.9406  Test accuracy= 0.946\n",
      "Epoch:  38   =====> Loss= 0.197312554  Validation accuracy= 0.9488  Test accuracy= 0.9447\n",
      "Epoch:  39   =====> Loss= 0.191021801  Validation accuracy= 0.9514  Test accuracy= 0.9473\n",
      "Epoch:  40   =====> Loss= 0.190293511  Validation accuracy= 0.9474  Test accuracy= 0.947\n",
      "It takes  627.7723133563995 to train\n",
      "Training Finished!\n",
      "Test accuracy: 0.9479\n",
      "It takes  1.1445341110229492 to test\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "val_hist, test_hist = train_DropOut(0.75 , learning_rate, training_epochs, batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "Epoch:  01   =====> Loss= 0.405675828  Validation accuracy= 0.9642  Test accuracy= 0.9632\n",
      "Epoch:  02   =====> Loss= 0.109867787  Validation accuracy= 0.9748  Test accuracy= 0.9706\n",
      "Epoch:  03   =====> Loss= 0.080308498  Validation accuracy= 0.978  Test accuracy= 0.9772\n",
      "Epoch:  04   =====> Loss= 0.062533649  Validation accuracy= 0.9816  Test accuracy= 0.9804\n",
      "Epoch:  05   =====> Loss= 0.057777045  Validation accuracy= 0.9824  Test accuracy= 0.9823\n",
      "Epoch:  06   =====> Loss= 0.050314618  Validation accuracy= 0.9838  Test accuracy= 0.984\n",
      "Epoch:  07   =====> Loss= 0.044219773  Validation accuracy= 0.9848  Test accuracy= 0.984\n",
      "Epoch:  08   =====> Loss= 0.041478408  Validation accuracy= 0.9834  Test accuracy= 0.9839\n",
      "Epoch:  09   =====> Loss= 0.033542314  Validation accuracy= 0.9842  Test accuracy= 0.9846\n",
      "Epoch:  10   =====> Loss= 0.032803432  Validation accuracy= 0.988  Test accuracy= 0.9888\n",
      "Epoch:  11   =====> Loss= 0.030590624  Validation accuracy= 0.9864  Test accuracy= 0.9873\n",
      "Epoch:  12   =====> Loss= 0.027908884  Validation accuracy= 0.9858  Test accuracy= 0.9867\n",
      "Epoch:  13   =====> Loss= 0.024847437  Validation accuracy= 0.9884  Test accuracy= 0.9871\n",
      "Epoch:  14   =====> Loss= 0.024827883  Validation accuracy= 0.9866  Test accuracy= 0.9871\n",
      "Epoch:  15   =====> Loss= 0.021737448  Validation accuracy= 0.9856  Test accuracy= 0.9857\n",
      "Epoch:  16   =====> Loss= 0.022121023  Validation accuracy= 0.9856  Test accuracy= 0.986\n",
      "Epoch:  17   =====> Loss= 0.020235663  Validation accuracy= 0.988  Test accuracy= 0.9889\n",
      "Epoch:  18   =====> Loss= 0.018655069  Validation accuracy= 0.9878  Test accuracy= 0.9879\n",
      "Epoch:  19   =====> Loss= 0.017375267  Validation accuracy= 0.9874  Test accuracy= 0.9866\n",
      "Epoch:  20   =====> Loss= 0.016626352  Validation accuracy= 0.988  Test accuracy= 0.9868\n",
      "Epoch:  21   =====> Loss= 0.016487425  Validation accuracy= 0.9876  Test accuracy= 0.9888\n",
      "Epoch:  22   =====> Loss= 0.015653011  Validation accuracy= 0.99  Test accuracy= 0.9874\n",
      "Validation Accuracy over 99% reached after 22 epochs\n",
      "It takes  349.49279379844666 to train\n",
      "Training Finished!\n",
      "Test accuracy: 0.9878\n",
      "It takes  1.135608196258545 to test\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "val_hist, test_hist = train_DropOut(0.75 , learning_rate, training_epochs, batch_size ,  optimizerF=\"Adam\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./MNIST_figures/TF_Data_With_Droupout.png\" width=\"600\" height=\"400\" align=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the on the two curves, the fastest one is the one with Adam and the other one without"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
